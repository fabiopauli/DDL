{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8DWFhojljZm1P7qsxWC82",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiopauli/DDL/blob/main/Qwen3_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Qwen3.5-27B API Server (llama.cpp + FastAPI + ngrok)\n",
        "\n",
        "Run **Qwen3.5-27B** (Dynamic 4-bit GGUF) on a **Google Colab L4 GPU** and expose it\n",
        "as an OpenAI-compatible API via **ngrok**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Prerequisites ‚Äî Colab Secrets\n",
        "\n",
        "Before running, add **two secrets** in the Colab sidebar (üîë icon ‚Üí \"Secrets\"):\n",
        "\n",
        "| Secret Name     | Where to get it                                | What it does                          |\n",
        "|-----------------|------------------------------------------------|---------------------------------------|\n",
        "| `NGROK_TOKEN`   | https://dashboard.ngrok.com/get-started/your-authtoken | Creates a public tunnel to your API  |\n",
        "| `HF_TOKEN`      | https://huggingface.co/settings/tokens         | Downloads gated/private models (optional for this model, but good practice) |\n",
        "\n",
        "> **Tip:** Toggle \"Notebook access\" ON for each secret after adding it.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã How to use\n",
        "\n",
        "1. Select **Runtime ‚Üí Change runtime type ‚Üí L4 GPU**\n",
        "2. Add your secrets (see above)\n",
        "3. Run **Cell 1** ‚Äî builds llama.cpp and downloads the model (~16 GB, takes ~5 min)\n",
        "4. Run **Cell 2** ‚Äî starts the FastAPI + ngrok server\n",
        "5. Copy the **ngrok URL** printed in the output\n",
        "6. Send requests to `<ngrok_url>/v1/chat/completions` (OpenAI-compatible)\n",
        "\n",
        "### Example request (Python)\n",
        "```python\n",
        "import requests\n",
        "\n",
        "URL = \"https://<your-ngrok-url>/v1/chat/completions\"\n",
        "\n",
        "response = requests.post(URL, json={\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Explain transformers in 3 sentences.\"}],\n",
        "    \"temperature\": 0.7,\n",
        "    \"max_tokens\": 512\n",
        "})\n",
        "print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
        "```\n",
        "\n",
        "### Example request (curl)\n",
        "```bash\n",
        "curl <ngrok_url>/v1/chat/completions \\\n",
        "  -H \"Content-Type: application/json\" \\\n",
        "  -d '{\"messages\":[{\"role\":\"user\",\"content\":\"Hello!\"}],\"temperature\":0.7}'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Notes\n",
        "- **Model:** `unsloth/Qwen3.5-27B-GGUF` (UD-Q4_K_XL ‚Äî Dynamic 4-bit)\n",
        "- **Context:** 16,384 tokens (model supports up to 256K, but VRAM-limited on L4)\n",
        "- **Mode:** Non-thinking (no `<think>` tags ‚Äî direct responses only)\n",
        "- **VRAM:** ~18-20 GB ‚Äî fits tightly on L4 (22.5 GB)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "QjgnuwUh-qCe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFk8FowM-oOd"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# CELL 1 ‚Äî Build llama.cpp + Download Model\n",
        "# =============================================================================\n",
        "\n",
        "# --- Build llama.cpp with CUDA ---\n",
        "!apt-get update -qq && apt-get install -qq -y pciutils build-essential cmake curl libcurl4-openssl-dev > /dev/null 2>&1\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp 2>/dev/null || echo \"Already cloned\"\n",
        "\n",
        "!cmake llama.cpp -B llama.cpp/build \\\n",
        "    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON > /dev/null 2>&1\n",
        "\n",
        "!cmake --build llama.cpp/build --config Release -j$(nproc) --clean-first \\\n",
        "    --target llama-server 2>&1 | tail -3\n",
        "\n",
        "!cp llama.cpp/build/bin/llama-* llama.cpp/\n",
        "\n",
        "# --- Download the GGUF model ---\n",
        "!pip install -q huggingface_hub hf_transfer\n",
        "\n",
        "import os\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# Use wget for reliability (huggingface-cli can hang in Colab)\n",
        "!mkdir -p unsloth/Qwen3.5-27B-GGUF\n",
        "!wget -c -q --show-progress \\\n",
        "    \"https://huggingface.co/unsloth/Qwen3.5-27B-GGUF/resolve/main/Qwen3.5-27B-UD-Q4_K_XL.gguf\" \\\n",
        "    -O unsloth/Qwen3.5-27B-GGUF/Qwen3.5-27B-UD-Q4_K_XL.gguf\n",
        "\n",
        "print(\"\\n‚úÖ Build and download complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 2 ‚Äî Start llama-server + FastAPI proxy + ngrok tunnel (non-blocking)\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q fastapi uvicorn pyngrok httpx\n",
        "\n",
        "import subprocess, time, threading, json, os\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import httpx\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import StreamingResponse, JSONResponse\n",
        "\n",
        "# ‚îÄ‚îÄ Config ‚îÄ‚îÄ\n",
        "NGROK_AUTH_TOKEN = userdata.get(\"NGROK_TOKEN\")\n",
        "MODEL_PATH = \"unsloth/Qwen3.5-27B-GGUF/Qwen3.5-27B-UD-Q4_K_XL.gguf\"\n",
        "LLAMA_PORT = 8081\n",
        "API_PORT = 8080\n",
        "CTX_SIZE = 16384\n",
        "\n",
        "# ‚îÄ‚îÄ 1. Start llama-server in background ‚îÄ‚îÄ\n",
        "llama_cmd = [\n",
        "    \"./llama.cpp/llama-server\",\n",
        "    \"--model\", MODEL_PATH,\n",
        "    \"--ctx-size\", str(CTX_SIZE),\n",
        "    \"--n-gpu-layers\", \"99\",\n",
        "    \"--port\", str(LLAMA_PORT),\n",
        "    \"--host\", \"0.0.0.0\",\n",
        "    \"--temp\", \"0.7\",\n",
        "    \"--top-p\", \"0.8\",\n",
        "    \"--top-k\", \"20\",\n",
        "    \"--min-p\", \"0.0\",\n",
        "    \"--chat-template-kwargs\", '{\"enable_thinking\": false}',\n",
        "]\n",
        "\n",
        "print(\"üîÑ Starting llama-server...\")\n",
        "llama_proc = subprocess.Popen(\n",
        "    llama_cmd,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        ")\n",
        "\n",
        "def stream_logs(proc):\n",
        "    for line in iter(proc.stdout.readline, b\"\"):\n",
        "        print(f\"  [llama] {line.decode().rstrip()}\")\n",
        "\n",
        "log_thread = threading.Thread(target=stream_logs, args=(llama_proc,), daemon=True)\n",
        "log_thread.start()\n",
        "\n",
        "# Wait for llama-server to be ready\n",
        "LLAMA_BASE = f\"http://127.0.0.1:{LLAMA_PORT}\"\n",
        "for i in range(120):\n",
        "    try:\n",
        "        r = httpx.get(f\"{LLAMA_BASE}/health\", timeout=2)\n",
        "        if r.status_code == 200:\n",
        "            print(\"‚úÖ llama-server is ready!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "else:\n",
        "    print(\"‚ùå llama-server failed to start. Check logs above.\")\n",
        "\n",
        "# ‚îÄ‚îÄ 2. FastAPI proxy ‚îÄ‚îÄ\n",
        "app = FastAPI(title=\"Qwen3.5-27B API\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    return {\"status\": \"ok\", \"model\": \"Qwen3.5-27B-UD-Q4_K_XL\"}\n",
        "\n",
        "@app.get(\"/v1/models\")\n",
        "async def models():\n",
        "    return {\n",
        "        \"object\": \"list\",\n",
        "        \"data\": [{\"id\": \"qwen3.5-27b\", \"object\": \"model\", \"owned_by\": \"unsloth\"}]\n",
        "    }\n",
        "\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def chat_completions(request: Request):\n",
        "    body = await request.json()\n",
        "    payload = {\n",
        "        \"messages\": body.get(\"messages\", []),\n",
        "        \"temperature\": body.get(\"temperature\", 0.7),\n",
        "        \"top_p\": body.get(\"top_p\", 0.8),\n",
        "        \"top_k\": body.get(\"top_k\", 20),\n",
        "        \"max_tokens\": body.get(\"max_tokens\", 2048),\n",
        "        \"stream\": body.get(\"stream\", False),\n",
        "    }\n",
        "    if payload[\"stream\"]:\n",
        "        async def event_stream():\n",
        "            async with httpx.AsyncClient() as client:\n",
        "                async with client.stream(\n",
        "                    \"POST\", f\"{LLAMA_BASE}/v1/chat/completions\",\n",
        "                    json=payload, timeout=300,\n",
        "                ) as resp:\n",
        "                    async for chunk in resp.aiter_bytes():\n",
        "                        yield chunk\n",
        "        return StreamingResponse(event_stream(), media_type=\"text/event-stream\")\n",
        "    else:\n",
        "        async with httpx.AsyncClient() as client:\n",
        "            resp = await client.post(\n",
        "                f\"{LLAMA_BASE}/v1/chat/completions\",\n",
        "                json=payload, timeout=300,\n",
        "            )\n",
        "            return JSONResponse(content=resp.json())\n",
        "\n",
        "# ‚îÄ‚îÄ 3. Start ngrok tunnel ‚îÄ‚îÄ\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "tunnel = ngrok.connect(API_PORT)\n",
        "public_url = tunnel.public_url\n",
        "\n",
        "# ‚îÄ‚îÄ 4. Run FastAPI in background thread ‚îÄ‚îÄ\n",
        "server_thread = threading.Thread(\n",
        "    target=uvicorn.run,\n",
        "    args=(app,),\n",
        "    kwargs={\"host\": \"0.0.0.0\", \"port\": API_PORT, \"log_level\": \"warning\"},\n",
        "    daemon=True,\n",
        ")\n",
        "server_thread.start()\n",
        "time.sleep(1)\n",
        "\n",
        "# Verify it's up\n",
        "try:\n",
        "    r = httpx.get(f\"http://127.0.0.1:{API_PORT}/health\", timeout=5)\n",
        "    assert r.status_code == 200\n",
        "    api_ok = \"‚úÖ\"\n",
        "except:\n",
        "    api_ok = \"‚ö†Ô∏è  (may need a moment)\"\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"üåê PUBLIC API URL: {public_url}\")\n",
        "print(f\"üè† LOCAL API URL:  http://127.0.0.1:{API_PORT}\")\n",
        "print(f\"   FastAPI: {api_ok}  |  llama-server: ‚úÖ\")\n",
        "print(f\"=\" * 60)\n",
        "print(f\"\\n  POST {public_url}/v1/chat/completions\")\n",
        "print(f\"\\n‚úÖ Server is running in background ‚Äî proceed to Cell 3!\")"
      ],
      "metadata": {
        "id": "9I-TCDnz_84O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# CELL 3 ‚Äî Interactive Chat (run after Cell 2 is running in another tab/cell)\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "import requests, json\n",
        "\n",
        "# ‚îÄ‚îÄ Config ‚îÄ‚îÄ\n",
        "# Replace with your ngrok URL from Cell 2 output, OR use localhost if same notebook with threaded server\n",
        "API_URL = \"http://127.0.0.1:8080/v1/chat/completions\"  # change to ngrok URL if external\n",
        "# API_URL = \"https://xxxx-xx-xx.ngrok-free.app/v1/chat/completions\"\n",
        "\n",
        "SYSTEM_PROMPT = \"You are a helpful assistant. Respond concisely and clearly.\"\n",
        "\n",
        "history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "\n",
        "def chat(user_msg, temperature=0.7, max_tokens=2048):\n",
        "    history.append({\"role\": \"user\", \"content\": user_msg})\n",
        "    try:\n",
        "        resp = requests.post(API_URL, json={\n",
        "            \"messages\": history,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens,\n",
        "        }, timeout=300)\n",
        "        resp.raise_for_status()\n",
        "        assistant_msg = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "        history.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "        return assistant_msg\n",
        "    except Exception as e:\n",
        "        history.pop()  # remove failed user msg\n",
        "        return f\"‚ùå Error: {e}\"\n",
        "\n",
        "# ‚îÄ‚îÄ Interactive loop ‚îÄ‚îÄ\n",
        "print(\"=\" * 60)\n",
        "print(\"üí¨ Qwen3.5-27B Chat ‚Äî type 'quit' to exit, 'clear' to reset\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        user_input = input(\"\\nüë§ You: \").strip()\n",
        "    except (KeyboardInterrupt, EOFError):\n",
        "        print(\"\\nüëã Bye!\")\n",
        "        break\n",
        "\n",
        "    if not user_input:\n",
        "        continue\n",
        "    if user_input.lower() == \"quit\":\n",
        "        print(\"üëã Bye!\")\n",
        "        break\n",
        "    if user_input.lower() == \"clear\":\n",
        "        history = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "        print(\"üóëÔ∏è  History cleared.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nü§ñ Qwen: \", end=\"\", flush=True)\n",
        "    reply = chat(user_input)\n",
        "    print(reply)"
      ],
      "metadata": {
        "id": "jwR8lAZSAC8d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}