{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNck5QCzFRVv0FpBBl8Zr5E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiopauli/Qwen3.5-colab/blob/main/Server_Qwen27B_llamacpp_256k_context_L4_20gb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Robust LLM API Server: Qwen3.5-27B + FastAPI Task Queue + Cloudflare\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/SEU_USUARIO/SEU_REPOSITORIO/blob/main/NOME_DO_NOTEBOOK.ipynb)\n",
        "\n",
        "Este notebook transforma o Google Colab em um **servidor de infer√™ncia de IA robusto e ass√≠ncrono**, ideal para integra√ß√µes com aplica√ß√µes externas (backends, automa√ß√µes, bots) que precisam lidar com m√∫ltiplas requisi√ß√µes sem sofrer com *timeouts*.\n",
        "\n",
        "## üß† O Modelo\n",
        "Estamos utilizando o **Qwen3.5-27B** (vers√£o GGUF quantizada pela Unsloth). √â um modelo extremamente capaz, rodando de forma otimizada via `llama.cpp` utilizando acelera√ß√£o por GPU (CUDA).\n",
        "\n",
        "> ‚ö†Ô∏è **Requisito de Hardware:** Para rodar este modelo de 27 bilh√µes de par√¢metros adequadamente, certifique-se de alterar o ambiente de execu√ß√£o do Colab para **GPU L4** (Preferencial) ou **A100**.\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è A Arquitetura: Filas (Queueing) e Polling\n",
        "\n",
        "Diferente de APIs tradicionais (onde o cliente faz a requisi√ß√£o, a conex√£o fica aberta, e ele espera a resposta de forma bloqueante), este servidor implementa um padr√£o de **Filas Ass√≠ncronas (Task Queue)**.\n",
        "\n",
        "**Por que isso √© necess√°rio?**\n",
        "Gera√ß√£o de texto em LLMs pode demorar minutos dependendo do tamanho do prompt. Requisi√ß√µes HTTP tradicionais costumam dar *timeout* ap√≥s 30 ou 60 segundos. Al√©m disso, se 5 usu√°rios pedirem textos ao mesmo tempo, o Colab ficaria sem mem√≥ria (OOM - Out of Memory).\n",
        "\n",
        "**Como funciona agora:**\n",
        "1. **Envio (Queueing):** O cliente envia o *prompt*. A API salva o pedido em uma fila, gera um **ID √∫nico** e responde instantaneamente: *\"Recebi seu pedido, ele est√° na fila\"*.\n",
        "2. **Processamento (Worker):** Em *background*, um *worker* pega uma tarefa da fila por vez e envia para o modelo procesar, protegendo a GPU contra sobrecarga.\n",
        "3. **Consulta (Polling):** O cliente usa o **ID √∫nico** para consultar a API periodicamente: *\"A tarefa j√° acabou?\"*. Quando o status mudar para `finished`, a resposta completa ser√° entregue.\n",
        "\n",
        "---\n",
        "\n",
        "## üîå Documenta√ß√£o da API\n",
        "\n",
        "Ap√≥s rodar todas as c√©lulas de infraestrutura, um t√∫nel do **Cloudflare** ser√° gerado com uma URL p√∫blica (ex: `https://seu-tunel.trycloudflare.com/v1`).\n",
        "\n",
        "### 1. Criar uma Tarefa (POST)\n",
        "**Endpoint:** `/v1/chat/completions`\n",
        "\n",
        "```json\n",
        "// Request Body\n",
        "{\n",
        "  \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "  \"messages\": [{\"role\": \"user\", \"content\": \"Seu prompt aqui\"}]\n",
        "}\n",
        "\n",
        "// Response (HTTP 202 Accepted)\n",
        "{\n",
        "  \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n",
        "  \"status\": \"queued\"\n",
        "}\n",
        "\n",
        "2. Consultar Status e Resultado (GET)\n",
        "Endpoint: /v1/tasks/{id}\n",
        "code\n",
        "JSON\n",
        "// Response enquato aguarda (HTTP 200)\n",
        "{\n",
        "  \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n",
        "  \"status\": \"processing\", // ou \"queued\"\n",
        "  \"result\": null,\n",
        "  \"error\": null\n",
        "}\n",
        "\n",
        "// Response quando conclu√≠do (HTTP 200)\n",
        "{\n",
        "  \"id\": \"123e4567-e89b-12d3-a456-426614174000\",\n",
        "  \"status\": \"finished\",\n",
        "  \"result\": {\n",
        "      \"id\": \"chatcmpl-...\",\n",
        "      \"choices\": [\n",
        "          {\"message\": {\"content\": \"Resposta gerada pela IA...\"}}\n",
        "      ]\n",
        "  },\n",
        "  \"error\": null\n",
        "}\n",
        "üõ†Ô∏è Passo a Passo para Uso\n",
        "C√©lula 1: Baixa as depend√™ncias, compila o llama.cpp com suporte a CUDA e baixa o modelo Qwen3.5-27B (Demora cerca de 5 a 8 minutos).\n",
        "C√©lula 2: Inicia o servidor base do llama.cpp em background.\n",
        "C√©lula 3: Instala as depend√™ncias do Python (FastAPI, Uvicorn, etc).\n",
        "C√©lula 4: Inicia o servidor de Fila Ass√≠ncrona (FastAPI) e cria o t√∫nel p√∫blico do Cloudflare.\n",
        "C√©lula 5: Um script Python pr√°tico que demonstra como fazer requisi√ß√µes ass√≠ncronas enviando m√∫ltiplas tarefas simult√¢neas e fazendo o polling para coletar os resultados.\n",
        "Aviso: As URLs geradas pelo Cloudflare Try s√£o ef√™meras e duram apenas enquanto a sess√£o do Colab estiver ativa."
      ],
      "metadata": {
        "id": "lp3ouRiOicI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Celula abaixo demora 8 minutos para ser conclu√≠da"
      ],
      "metadata": {
        "id": "E564goWkNeir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Build llama.cpp with CUDA and run Qwen3.5-27B (non-thinking mode)\n",
        "!apt-get update -qq && apt-get install -qq -y pciutils build-essential cmake curl libcurl4-openssl-dev > /dev/null 2>&1\n",
        "\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp 2>/dev/null || echo \"already cloned\"\n",
        "\n",
        "!cmake llama.cpp -B llama.cpp/build \\\n",
        "    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON > /dev/null 2>&1\n",
        "\n",
        "!cmake --build llama.cpp/build --config Release -j$(nproc) --clean-first --target llama-cli llama-server 2>&1 | tail -5\n",
        "\n",
        "!cp llama.cpp/build/bin/llama-* llama.cpp/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAd6icy52eED",
        "outputId": "08d6b647-05e3-4b8a-e3dd-e82c61fa2cc4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\n",
            "[100%] Linking CXX executable ../../bin/llama-server\n",
            "[100%] Built target llama-server\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h/bin/bash: line 1: huggingface-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LsSf https://hf.co/cli/install.sh | bash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1R82w3H37On",
        "outputId": "3bcbbc52-4715-495c-c9e3-4283260a94a5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0;34m[INFO]\u001b[0m Installing Hugging Face CLI...\n",
            "\u001b[0;34m[INFO]\u001b[0m OS: linux\n",
            "\u001b[0;34m[INFO]\u001b[0m Force reinstall: false\n",
            "\u001b[0;34m[INFO]\u001b[0m Install dir: /root/.hf-cli\n",
            "\u001b[0;34m[INFO]\u001b[0m Bin dir: /root/.local/bin\n",
            "\u001b[0;34m[INFO]\u001b[0m Skip PATH update: false\n",
            "\u001b[0;34m[INFO]\u001b[0m Using Python: Python 3.12.12\n",
            "\u001b[0;34m[INFO]\u001b[0m Creating directories...\n",
            "\u001b[0;34m[INFO]\u001b[0m Creating virtual environment...\n",
            "\u001b[0;34m[INFO]\u001b[0m Virtual environment already exists; reusing (pass --force to recreate)\n",
            "\u001b[0;34m[INFO]\u001b[0m Installing/upgrading Hugging Face CLI (latest)...\n",
            "\u001b[0;34m[INFO]\u001b[0m Installation output suppressed; set HF_CLI_VERBOSE_PIP=1 for full logs\n",
            "\u001b[0;34m[INFO]\u001b[0m Using uv for faster installation\n",
            "\u001b[0;34m[INFO]\u001b[0m Linking hf CLI into /root/.local/bin...\n",
            "\u001b[0;34m[INFO]\u001b[0m hf available at /root/.local/bin/hf (symlink to venv)\n",
            "\u001b[0;34m[INFO]\u001b[0m Run without touching PATH: env PATH=\"/root/.local/bin:$PATH\" hf --help\n",
            "\u001b[0;34m[INFO]\u001b[0m /root/.local/bin is not in your PATH\n",
            "\u001b[0;32m[SUCCESS]\u001b[0m Added /root/.local/bin to PATH via /root/.bashrc\n",
            "\u001b[0;34m[INFO]\u001b[0m Apply it now with: source /root/.bashrc\n",
            "\u001b[0;34m[INFO]\u001b[0m Verifying installation...\n",
            "\u001b[0;32m[SUCCESS]\u001b[0m Hugging Face CLI installed successfully!\n",
            "\u001b[0;34m[INFO]\u001b[0m CLI location: /root/.local/bin/hf\n",
            "\u001b[0;34m[INFO]\u001b[0m Installation directory: /root/.hf-cli\n",
            "\u001b[0;34m[INFO]\u001b[0m Current version: 1.4.1\n",
            "\u001b[0;34m[INFO]\u001b[0m \n",
            "\u001b[0;34m[INFO]\u001b[0m To uninstall the Hugging Face CLI, run:\n",
            "\u001b[0;34m[INFO]\u001b[0m   rm -rf /root/.hf-cli\n",
            "\u001b[0;34m[INFO]\u001b[0m   rm -f /root/.local/bin/hf\n",
            "\u001b[0;34m[INFO]\u001b[0m \n",
            "\u001b[0;34m[INFO]\u001b[0m   (shell) Undo PATH entry: sed -i.bak '/Added by Hugging Face CLI installer/d' /root/.bashrc && rm -f /root/.bashrc.bak\n",
            "\u001b[0;32m[SUCCESS]\u001b[0m hf CLI ready!\n",
            "\u001b[0;34m[INFO]\u001b[0m Binary: /root/.local/bin/hf\n",
            "\u001b[0;34m[INFO]\u001b[0m Virtualenv: /root/.hf-cli\n",
            "\u001b[0;34m[INFO]\u001b[0m Try it now: env PATH=\"/root/.local/bin:$PATH\" hf --help\n",
            "\u001b[0;34m[INFO]\u001b[0m Examples:\n",
            "\u001b[0;34m[INFO]\u001b[0m   hf auth login\n",
            "\u001b[0;34m[INFO]\u001b[0m   hf download deepseek-ai/DeepSeek-R1\n",
            "\u001b[0;34m[INFO]\u001b[0m   hf jobs run python:3.12 python -c 'print(\"Hello from HF CLI!\")'\n",
            "\u001b[0;34m[INFO]\u001b[0m \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv venv /root/.hf-cli/venv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUnqPyK54F3o",
        "outputId": "3a2dce16-0d98-40b8-a20c-bc841582a45b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, `uv venv` always ignores virtual environments when finding a Python interpreter; did you mean `--no-managed-python`?\u001b[0m\n",
            "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
            "Creating virtual environment at: \u001b[36m/root/.hf-cli/venv\u001b[39m\n",
            "\u001b[33m?\u001b[0m \u001b[1mA virtual environment already exists at `/root/.hf-cli/venv`. Do you want to replace it?\u001b[0m \u001b[38;5;8m[y/n]\u001b[0m \u001b[38;5;8m‚Ä∫\u001b[0m \u001b[36myes\u001b[0m\n",
            "\n",
            "\u001b[36m\u001b[1mhint\u001b[0m\u001b[1m:\u001b[0m Use the `\u001b[32m--clear\u001b[39m` flag or set `\u001b[32mUV_VENV_CLEAR=1\u001b[39m` to skip this prompt\u001b[?25l^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hf download unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --local-dir unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --include \"*UD-Q4_K_XL*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXRy_7Mu4gDY",
        "outputId": "6ca84afe-771f-4f5a-b5c9-5991ee1408e3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
            "Downloading (incomplete total...): 100% 16.7G/16.7G [00:55<00:00, 374MB/s]\n",
            "Fetching 1 files: 100% 1/1 [00:55<00:00, 55.60s/it]\n",
            "Download complete: 100% 16.7G/16.7G [00:55<00:00, 374MB/s]                /content/unsloth/Qwen3.5-27B-GGUF\n",
            "Download complete: 100% 16.7G/16.7G [00:55<00:00, 301MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A c√©lula abaixo cria o servidor Llamacpp em background."
      ],
      "metadata": {
        "id": "nwbeV8MTNn0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Run llama-server in the background\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Kill any existing server to free up the port\n",
        "os.system(\"pkill -f llama-server\")\n",
        "time.sleep(2)\n",
        "\n",
        "os.environ[\"LLAMA_CACHE\"] = \"unsloth/Qwen3.5-27B-GGUF\"\n",
        "\n",
        "# Start the server using nohup so it runs in the background\n",
        "server_cmd = \"\"\"\n",
        "nohup ./llama.cpp/llama-server \\\n",
        "    -hf unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL \\\n",
        "    --host 127.0.0.1 \\\n",
        "    --port 8081 \\\n",
        "    --ctx-size 32768 \\\n",
        "    -ngl 99 \\\n",
        "    --temp 0.7 \\\n",
        "    --top-p 0.8 \\\n",
        "    --top-k 20 \\\n",
        "    --min-p 0.00 \\\n",
        "    --chat-template-kwargs '{\"enable_thinking\": false}' \\\n",
        "    --cache-type-k q8_0 \\\n",
        "    --cache-type-v q8_0 > llama_server.log 2>&1 &\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting llama-server on port 8081...\")\n",
        "os.system(server_cmd)\n",
        "\n",
        "# Wait for the server to spin up and load the model into VRAM\n",
        "print(\"Waiting for model to load into VRAM (this takes 30-60 seconds)...\")\n",
        "for i in range(600):\n",
        "    try:\n",
        "        import requests\n",
        "        res = requests.get(\"http://127.0.0.1:8081/health\")\n",
        "        if res.status_code == 200:\n",
        "            print(\"\\n‚úÖ llama-server is ready and listening on port 8081!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(2)\n",
        "    print(\".\", end=\"\", flush=True)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Server might not have started correctly. Check llama_server.log:\")\n",
        "    os.system(\"tail -n 20 llama_server.log\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rp-w75DFtw1",
        "outputId": "29b87e52-781b-4ecc-8729-26e49f7071a7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting llama-server on port 8081...\n",
            "Waiting for model to load into VRAM (this takes 30-60 seconds)...\n",
            "...............................................................................................................................................................................................\n",
            "‚úÖ llama-server is ready and listening on port 8081!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir, criamos outro servidor para gerar os endpoints da API, tamb√©m em background"
      ],
      "metadata": {
        "id": "kmwz2laJSJfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install dependencies for FastAPI wrapper\n",
        "!pip install -q fastapi uvicorn pyngrok httpx pydantic nest-asyncio slowapi"
      ],
      "metadata": {
        "id": "gxYG3JoJ0yU2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349a25ea-092f-47e5-db1b-3230d056f86c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C√©lula 4 - servidor robusto, com rate limiting"
      ],
      "metadata": {
        "id": "BEks5d6GxSC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Robust Background FastAPI (Queue + Rate Limits + Safe Tokenizer)\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# 1. Prepare the FastAPI code\n",
        "fastapi_code = \"\"\"\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import uuid\n",
        "import time\n",
        "from fastapi import FastAPI, Request, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import httpx\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# --- Imports for Rate Limiting ---\n",
        "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
        "from slowapi.errors import RateLimitExceeded\n",
        "\n",
        "# --- ROBUST TOKENIZER SETUP ---\n",
        "# We wrap this in try/except so the server NEVER crashes due to tiktoken\n",
        "encoding = None\n",
        "try:\n",
        "    import tiktoken\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    print(\"‚úÖ Tiktoken loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Tiktoken failed to load ({e}). Using fallback estimation.\")\n",
        "    encoding = None\n",
        "\n",
        "MAX_TOKEN_LIMIT = 64000\n",
        "\n",
        "# --- Helper to get Real IP ---\n",
        "def get_real_ip(request: Request) -> str:\n",
        "    cf_ip = request.headers.get(\"cf-connecting-ip\")\n",
        "    return cf_ip if cf_ip else \"127.0.0.1\"\n",
        "\n",
        "# --- Config Limiter ---\n",
        "limiter = Limiter(key_func=get_real_ip)\n",
        "app = FastAPI(title=\"Robust Queued FastAPI\")\n",
        "app.state.limiter = limiter\n",
        "app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "LLAMA_SERVER_URL = \"http://127.0.0.1:8081\"\n",
        "tasks_db: Dict[str, Dict[str, Any]] = {}\n",
        "request_queue = asyncio.Queue(maxsize=100)\n",
        "\n",
        "# --- Token Counting Function (Safe Version) ---\n",
        "def count_tokens_safe(messages: List[Dict[str, str]]) -> int:\n",
        "    text_buffer = \"\"\n",
        "    for m in messages:\n",
        "        text_buffer += m.get(\"content\", \"\")\n",
        "\n",
        "    if encoding:\n",
        "        return len(encoding.encode(text_buffer))\n",
        "    else:\n",
        "        # Fallback: ~4 characters per token\n",
        "        return len(text_buffer) // 4\n",
        "\n",
        "# --- Worker ---\n",
        "async def process_queue():\n",
        "    async with httpx.AsyncClient(timeout=600.0) as client:\n",
        "        while True:\n",
        "            task_id, payload = await request_queue.get()\n",
        "            if task_id not in tasks_db:\n",
        "                request_queue.task_done()\n",
        "                continue\n",
        "\n",
        "            tasks_db[task_id][\"status\"] = \"processing\"\n",
        "            try:\n",
        "                payload[\"stream\"] = False\n",
        "                response = await client.post(\n",
        "                    f\"{LLAMA_SERVER_URL}/v1/chat/completions\",\n",
        "                    json=payload\n",
        "                )\n",
        "                if response.status_code != 200:\n",
        "                    tasks_db[task_id][\"status\"] = \"failed\"\n",
        "                    tasks_db[task_id][\"error\"] = f\"Upstream: {response.text}\"\n",
        "                else:\n",
        "                    tasks_db[task_id][\"status\"] = \"finished\"\n",
        "                    tasks_db[task_id][\"result\"] = response.json()\n",
        "            except Exception as e:\n",
        "                tasks_db[task_id][\"status\"] = \"failed\"\n",
        "                tasks_db[task_id][\"error\"] = str(e)\n",
        "            finally:\n",
        "                request_queue.task_done()\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    asyncio.create_task(process_queue())\n",
        "\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "@limiter.limit(\"10/minute\")\n",
        "async def queue_chat_completion(request: Request):\n",
        "    try:\n",
        "        payload = await request.json()\n",
        "    except:\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid JSON\")\n",
        "\n",
        "    messages = payload.get(\"messages\", [])\n",
        "\n",
        "    # 1. Count Tokens\n",
        "    token_count = count_tokens_safe(messages)\n",
        "\n",
        "    # 2. Reject if too big\n",
        "    if token_count > MAX_TOKEN_LIMIT:\n",
        "        raise HTTPException(status_code=400, detail=f\"Context too long: {token_count} > {MAX_TOKEN_LIMIT}\")\n",
        "\n",
        "    # 3. Queue\n",
        "    task_id = str(uuid.uuid4())\n",
        "    tasks_db[task_id] = {\n",
        "        \"id\": task_id, \"status\": \"queued\",\n",
        "        \"created_at\": time.time(), \"tokens\": token_count\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        request_queue.put_nowait((task_id, payload))\n",
        "    except asyncio.QueueFull:\n",
        "        del tasks_db[task_id]\n",
        "        raise HTTPException(status_code=503, detail=\"Queue full\")\n",
        "\n",
        "    return JSONResponse(content={\"id\": task_id, \"status\": \"queued\"}, status_code=202)\n",
        "\n",
        "@app.get(\"/v1/tasks/{task_id}\")\n",
        "@limiter.limit(\"60/minute\")\n",
        "async def get_task_status(request: Request, task_id: str):\n",
        "    if task_id not in tasks_db:\n",
        "        raise HTTPException(status_code=404, detail=\"Task not found\")\n",
        "    return tasks_db[task_id]\n",
        "\n",
        "# Endpoint for CRON deletion\n",
        "@app.delete(\"/v1/tasks/cleanup\")\n",
        "async def cleanup(request: Request, older_than: int = 3600):\n",
        "    now = time.time()\n",
        "    to_del = [k for k, v in tasks_db.items() if now - v.get(\"created_at\", 0) > older_than]\n",
        "    for k in to_del: del tasks_db[k]\n",
        "    return {\"deleted\": len(to_del)}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fastapi_server.py\", \"w\") as f:\n",
        "    f.write(fastapi_code)\n",
        "\n",
        "# 2. Cleanup old processes\n",
        "os.system(\"pkill -f uvicorn\")\n",
        "os.system(\"pkill -f cloudflared\")\n",
        "time.sleep(2)\n",
        "\n",
        "# 3. Start FastAPI and LOG OUTPUT\n",
        "print(\"üöÄ Starting FastAPI...\")\n",
        "# We redirect stderr to stdout to catch python errors\n",
        "os.system(\"nohup python -m uvicorn fastapi_server:app --host 0.0.0.0 --port 8000 > fastapi.log 2>&1 &\")\n",
        "\n",
        "# 4. Wait and Check if it crashed\n",
        "time.sleep(5)\n",
        "with open(\"fastapi_server.py\", \"r\") as f:\n",
        "    pass # Just checking file exists\n",
        "\n",
        "# Check logs to see if it's actually running\n",
        "with open(\"fastapi.log\", \"r\") as f:\n",
        "    log_content = f.read()\n",
        "    if \"Application startup complete\" not in log_content:\n",
        "        print(\"\\n‚ùå CRITICAL ERROR: FastAPI failed to start!\")\n",
        "        print(\"--- LOG START ---\")\n",
        "        print(log_content)\n",
        "        print(\"--- LOG END ---\")\n",
        "        raise RuntimeError(\"Fix the errors above before starting Cloudflare.\")\n",
        "    else:\n",
        "        print(\"‚úÖ FastAPI started successfully.\")\n",
        "\n",
        "# 5. Start Cloudflare Tunnel\n",
        "print(\"üîó Starting Cloudflare Tunnel...\")\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    os.system(\"wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\")\n",
        "    os.system(\"chmod +x cloudflared\")\n",
        "\n",
        "os.system(\"nohup ./cloudflared tunnel --url http://127.0.0.1:8000 > cloudflare.log 2>&1 &\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 6. Get URL\n",
        "with open(\"cloudflare.log\", \"r\") as f:\n",
        "    logs = f.read()\n",
        "    match = re.search(r\"(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)\", logs)\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        base_url = f\"{public_url}/v1\"\n",
        "        with open(\"api_url.txt\", \"w\") as url_file:\n",
        "            url_file.write(base_url)\n",
        "        print(f\"\\n‚úÖ URL saved: {base_url}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find Cloudflare URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-PPPbXqzQwW",
        "outputId": "490fa043-2e5b-4d62-db61-25066131b2bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting FastAPI...\n",
            "‚úÖ FastAPI started successfully.\n",
            "üîó Starting Cloudflare Tunnel...\n",
            "\n",
            "‚úÖ URL saved: https://trek-ken-doing-attended.trycloudflare.com/v1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test da API"
      ],
      "metadata": {
        "id": "KcncMmJB246h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Test Suite (Queue, Token Limits, and Rate Limiting)\n",
        "import requests\n",
        "import time\n",
        "import sys\n",
        "\n",
        "# 1. Load the API URL\n",
        "try:\n",
        "    with open(\"api_url.txt\", \"r\") as f:\n",
        "        BASE_URL = f.read().strip()\n",
        "    print(f\"üéØ Targeting API at: {BASE_URL}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: 'api_url.txt' not found. Run the server cell first.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "def poll_task(task_id):\n",
        "    \"\"\"Helper to poll for completion\"\"\"\n",
        "    print(f\"   ‚è≥ Polling task {task_id}...\", end=\"\", flush=True)\n",
        "    for _ in range(30): # Wait up to 60 seconds\n",
        "        try:\n",
        "            res = requests.get(f\"{BASE_URL}/tasks/{task_id}\")\n",
        "            if res.status_code == 429:\n",
        "                print(\" (Rate limited on polling) \", end=\"\")\n",
        "                time.sleep(5)\n",
        "                continue\n",
        "\n",
        "            data = res.json()\n",
        "            status = data.get(\"status\")\n",
        "            if status == \"finished\":\n",
        "                print(f\"\\n   ‚úÖ Completed! Response: {data['result']['choices'][0]['message']['content'][:50]}...\")\n",
        "                return True\n",
        "            elif status == \"failed\":\n",
        "                print(f\"\\n   ‚ùå Task Failed: {data.get('error')}\")\n",
        "                return False\n",
        "            else:\n",
        "                print(\".\", end=\"\", flush=True)\n",
        "                time.sleep(2)\n",
        "        except Exception as e:\n",
        "            print(f\" Error: {e}\")\n",
        "            return False\n",
        "    print(\"\\n   ‚ö†Ô∏è Polling timed out.\")\n",
        "    return False\n",
        "\n",
        "# --- TEST 1: The \"Happy Path\" ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 1: Standard Request (Queue System)\")\n",
        "payload_normal = {\n",
        "    \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"What is 2+2? Answer in one word.\"}]\n",
        "}\n",
        "resp = requests.post(f\"{BASE_URL}/chat/completions\", json=payload_normal)\n",
        "\n",
        "if resp.status_code == 202:\n",
        "    data = resp.json()\n",
        "    t_id = data['id']\n",
        "    print(f\"‚úÖ Request Queued. ID: {t_id}\")\n",
        "    poll_task(t_id)\n",
        "else:\n",
        "    print(f\"‚ùå Failed: {resp.status_code} - {resp.text}\")\n",
        "\n",
        "\n",
        "# --- TEST 2: The \"Context Limit\" (Rejection > 64k) ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 2: Context Window Limit (64k Tokens)\")\n",
        "# Create a massive string. 'test ' is 1 token. 70,000 repeats > 64k limit.\n",
        "huge_content = \"test \" * 70000\n",
        "payload_huge = {\n",
        "    \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": huge_content}]\n",
        "}\n",
        "print(f\"   üì§ Sending payload with ~70,000 tokens...\")\n",
        "resp = requests.post(f\"{BASE_URL}/chat/completions\", json=payload_huge)\n",
        "\n",
        "if resp.status_code == 400:\n",
        "    print(f\"‚úÖ Success! Server rejected the request.\")\n",
        "    print(f\"   Response: {resp.json()['detail']}\")\n",
        "elif resp.status_code == 202:\n",
        "    print(f\"‚ùå Fail: Server accepted the huge request (Limit didn't work).\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Unexpected status: {resp.status_code}\")\n",
        "\n",
        "\n",
        "# --- TEST 3: Rate Limiting (Spamming) ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TEST 3: Rate Limiting (Max 10/min)\")\n",
        "print(\"   üöÄ Spamming requests to trigger 429...\")\n",
        "\n",
        "limit_hit = False\n",
        "for i in range(1, 15):\n",
        "    payload = {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}\n",
        "    resp = requests.post(f\"{BASE_URL}/chat/completions\", json=payload)\n",
        "\n",
        "    if resp.status_code == 429:\n",
        "        print(f\"\\n‚úÖ Rate Limit Triggered on request #{i}!\")\n",
        "        print(f\"   Server said: {resp.text}\")\n",
        "        limit_hit = True\n",
        "        break\n",
        "    else:\n",
        "        print(f\"   Request {i}: {resp.status_code}\", end=\"\\r\")\n",
        "        time.sleep(0.1)\n",
        "\n",
        "if not limit_hit:\n",
        "    print(\"\\n‚ùå Failed to trigger rate limit. (Did you wait a minute since the last test?)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdhkuy2RyIT7",
        "outputId": "fec65333-4738-4fe6-ce37-0f3192d9f5fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Targeting API at: https://trek-ken-doing-attended.trycloudflare.com/v1\n",
            "\n",
            "==================================================\n",
            "TEST 1: Standard Request (Queue System)\n",
            "‚ùå Failed: 530 - <!DOCTYPE html>\n",
            "<!--[if lt IE 7]> <html class=\"no-js ie6 oldie\" lang=\"en-US\"> <![endif]-->\n",
            "<!--[if IE 7]>    <html class=\"no-js ie7 oldie\" lang=\"en-US\"> <![endif]-->\n",
            "<!--[if IE 8]>    <html class=\"no-js ie8 oldie\" lang=\"en-US\"> <![endif]-->\n",
            "<!--[if gt IE 8]><!--> <html class=\"no-js\" lang=\"en-US\"> <!--<![endif]-->\n",
            "<head>\n",
            "<title>Cloudflare Tunnel error | trek-ken-doing-attended.trycloudflare.com | Cloudflare</title>\n",
            "<meta charset=\"UTF-8\" />\n",
            "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
            "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" />\n",
            "<meta name=\"robots\" content=\"noindex, nofollow\" />\n",
            "<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
            "<link rel=\"stylesheet\" id=\"cf_styles-css\" href=\"/cdn-cgi/styles/main.css\" />\n",
            "\n",
            "\n",
            "<script>\n",
            "(function(){if(document.addEventListener&&window.XMLHttpRequest&&JSON&&JSON.stringify){var e=function(a){var c=document.getElementById(\"error-feedback-survey\"),d=document.getElementById(\"error-feedback-success\"),b=new XMLHttpRequest;a={event:\"feedback clicked\",properties:{errorCode:1033,helpful:a,version:1}};b.open(\"POST\",\"https://sparrow.cloudflare.com/api/v1/event\");b.setRequestHeader(\"Content-Type\",\"application/json\");b.setRequestHeader(\"Sparrow-Source-Key\",\"c771f0e4b54944bebf4261d44bd79a1e\");\n",
            "b.send(JSON.stringify(a));c.classList.add(\"feedback-hidden\");d.classList.remove(\"feedback-hidden\")};document.addEventListener(\"DOMContentLoaded\",function(){var a=document.getElementById(\"error-feedback\"),c=document.getElementById(\"feedback-button-yes\"),d=document.getElementById(\"feedback-button-no\");\"classList\"in a&&(a.classList.remove(\"feedback-hidden\"),c.addEventListener(\"click\",function(){e(!0)}),d.addEventListener(\"click\",function(){e(!1)}))})}})();\n",
            "</script>\n",
            "\n",
            "<script defer src=\"https://performance.radar.cloudflare.com/beacon.js\"></script>\n",
            "</head>\n",
            "<body>\n",
            "  <div id=\"cf-wrapper\">\n",
            "    <div class=\"cf-alert cf-alert-error cf-cookie-error hidden\" id=\"cookie-alert\" data-translate=\"enable_cookies\">Please enable cookies.</div>\n",
            "    <div id=\"cf-error-details\" class=\"p-0\">\n",
            "      <header class=\"mx-auto pt-10 lg:pt-6 lg:px-8 w-240 lg:w-full mb-15 antialiased\">\n",
            "         <h1 class=\"inline-block md:block mr-2 md:mb-2 font-light text-60 md:text-3xl text-black-dark leading-tight\">\n",
            "           <span data-translate=\"error\">Error</span>\n",
            "           <span>1033</span>\n",
            "         </h1>\n",
            "         <span class=\"inline-block md:block heading-ray-id font-mono text-15 lg:text-sm lg:leading-relaxed\">Ray ID: 9d3f6e22b7b006b6 &bull;</span>\n",
            "         <span class=\"inline-block md:block heading-ray-id font-mono text-15 lg:text-sm lg:leading-relaxed\">2026-02-26 12:26:19 UTC</span>\n",
            "        <h2 class=\"text-gray-600 leading-1.3 text-3xl lg:text-2xl font-light\">Cloudflare Tunnel error</h2>\n",
            "      </header>\n",
            "\n",
            "      <section class=\"w-240 lg:w-full mx-auto mb-8 lg:px-8\">\n",
            "          <div id=\"what-happened-section\" class=\"w-1/2 md:w-full\">\n",
            "            <h2 class=\"text-3xl leading-tight font-normal mb-4 text-black-dark antialiased\" data-translate=\"what_happened\">What happened?</h2>\n",
            "            <p>You've requested a page on a website (trek-ken-doing-attended.trycloudflare.com) that is on the <a href=\"https://www.cloudflare.com/5xx-error-landing/\" target=\"_blank\">Cloudflare</a> network. The host (trek-ken-doing-attended.trycloudflare.com) is configured as a Cloudflare Tunnel, and Cloudflare is currently unable to resolve it.\n",
            "            \n",
            "          </div>\n",
            "\n",
            "          \n",
            "          <div id=\"resolution-copy-section\" class=\"w-1/2 mt-6 text-15 leading-normal\">\n",
            "            <h2 class=\"text-3xl leading-tight font-normal mb-4 text-black-dark antialiased\" data-translate=\"what_can_i_do\">What can I do?</h2>\n",
            "            <p><strong>If you are a visitor of this website:</strong><br />Please try again in a few minutes.</p><p><strong>If you are the owner of this website:</strong><br />Ensure that cloudflared is running and can reach the network. You may wish to enable <a rel=\"noopener noreferrer\" href=\"https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/routing-to-tunnel/lb/\">load balancing</a> for your tunnel.</p>\n",
            "          </div>\n",
            "          \n",
            "      </section>\n",
            "\n",
            "      <div class=\"feedback-hidden py-8 text-center\" id=\"error-feedback\">\n",
            "    <div id=\"error-feedback-survey\" class=\"footer-line-wrapper\">\n",
            "        Was this page helpful?\n",
            "        <button class=\"border border-solid bg-white cf-button cursor-pointer ml-4 px-4 py-2 rounded\" id=\"feedback-button-yes\" type=\"button\">Yes</button>\n",
            "        <button class=\"border border-solid bg-white cf-button cursor-pointer ml-4 px-4 py-2 rounded\" id=\"feedback-button-no\" type=\"button\">No</button>\n",
            "    </div>\n",
            "    <div class=\"feedback-success feedback-hidden\" id=\"error-feedback-success\">\n",
            "        Thank you for your feedback!\n",
            "    </div>\n",
            "</div>\n",
            "\n",
            "\n",
            "      <div class=\"cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300\">\n",
            "  <p class=\"text-13\">\n",
            "    <span class=\"cf-footer-item sm:block sm:mb-1\">Cloudflare Ray ID: <strong class=\"font-semibold\">9d3f6e22b7b006b6</strong></span>\n",
            "    <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
            "    <span id=\"cf-footer-item-ip\" class=\"cf-footer-item hidden sm:block sm:mb-1\">\n",
            "      Your IP:\n",
            "      <button type=\"button\" id=\"cf-footer-ip-reveal\" class=\"cf-footer-ip-reveal-btn\">Click to reveal</button>\n",
            "      <span class=\"hidden\" id=\"cf-footer-ip\">34.142.175.203</span>\n",
            "      <span class=\"cf-footer-separator sm:hidden\">&bull;</span>\n",
            "    </span>\n",
            "    <span class=\"cf-footer-item sm:block sm:mb-1\"><span>Performance &amp; security by</span> <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com/5xx-error-landing\" id=\"brand_link\" target=\"_blank\">Cloudflare</a></span>\n",
            "    \n",
            "  </p>\n",
            "  <script>(function(){function d(){var b=a.getElementById(\"cf-footer-item-ip\"),c=a.getElementById(\"cf-footer-ip-reveal\");b&&\"classList\"in b&&(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf-footer-ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&&a.addEventListener(\"DOMContentLoaded\",d)})();</script>\n",
            "</div><!-- /.error-footer -->\n",
            "\n",
            "\n",
            "    </div><!-- /#cf-error-details -->\n",
            "  </div><!-- /#cf-wrapper -->\n",
            "\n",
            "  <script>\n",
            "  window._cf_translation = {};\n",
            "  \n",
            "  \n",
            "</script>\n",
            "\n",
            "</body>\n",
            "</html>\n",
            "\n",
            "\n",
            "==================================================\n",
            "TEST 2: Context Window Limit (64k Tokens)\n",
            "   üì§ Sending payload with ~70,000 tokens...\n",
            "‚ö†Ô∏è Unexpected status: 530\n",
            "\n",
            "==================================================\n",
            "TEST 3: Rate Limiting (Max 10/min)\n",
            "   üöÄ Spamming requests to trigger 429...\n",
            "   Request 14: 530\n",
            "‚ùå Failed to trigger rate limit. (Did you wait a minute since the last test?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo de como consumir a API"
      ],
      "metadata": {
        "id": "q32ukQCG2_5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Consumer Client - Testing the Queue with Legal Prompts\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "try:\n",
        "    with open(\"api_url.txt\", \"r\") as f:\n",
        "        API_BASE_URL = f.read().strip()\n",
        "    print(f\"üîó Connected to: {API_BASE_URL}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: URL file not found. Run the server cell first.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# The prompts to test\n",
        "PROMPTS = [\n",
        "    \"Escreva um texto sobre direito previdenci√°rio\",\n",
        "    \"Escreva sobre o Supremo Tribunal Federal no Brasil\"\n",
        "]\n",
        "\n",
        "# Store task IDs here\n",
        "active_tasks = {}\n",
        "\n",
        "# --- STEP 1: SEND REQUESTS (Queueing) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üì§ STEP 1: SENDING REQUESTS TO QUEUE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, prompt in enumerate(PROMPTS):\n",
        "    print(f\"Sending prompt {i+1}: '{prompt}'...\")\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente jur√≠dico especialista em direito brasileiro. Responda de forma t√©cnica e completa.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.6,\n",
        "        \"max_tokens\": 2048 # Limit output length\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(f\"{API_BASE_URL}/chat/completions\", json=payload)\n",
        "\n",
        "        if response.status_code == 202:\n",
        "            data = response.json()\n",
        "            task_id = data[\"id\"]\n",
        "            active_tasks[task_id] = {\"prompt\": prompt, \"status\": \"queued\"}\n",
        "            print(f\"   ‚úÖ Queued! Task ID: {task_id}\")\n",
        "        elif response.status_code == 429:\n",
        "            print(\"   ‚ùå Rate Limited (Wait a minute and try again)\")\n",
        "        else:\n",
        "            print(f\"   ‚ùå Error {response.status_code}: {response.text}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Connection Error: {e}\")\n",
        "\n",
        "# --- STEP 2: POLLING LOOP (Waiting for results) ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚è≥ STEP 2: WAITING FOR GENERATION (POLLING)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "completed_results = {}\n",
        "\n",
        "while len(active_tasks) > 0:\n",
        "    # Iterate over a copy of keys so we can remove finished tasks safely\n",
        "    current_ids = list(active_tasks.keys())\n",
        "\n",
        "    for task_id in current_ids:\n",
        "        try:\n",
        "            # Check status\n",
        "            res = requests.get(f\"{API_BASE_URL}/tasks/{task_id}\")\n",
        "\n",
        "            if res.status_code == 200:\n",
        "                data = res.json()\n",
        "                status = data.get(\"status\")\n",
        "\n",
        "                # Update status for display\n",
        "                if active_tasks[task_id][\"status\"] != status:\n",
        "                    print(f\"Task {task_id[:8]}... status changed to: {status.upper()}\")\n",
        "                    active_tasks[task_id][\"status\"] = status\n",
        "\n",
        "                if status == \"finished\":\n",
        "                    # Save result and remove from active list\n",
        "                    content = data[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "                    completed_results[task_id] = {\n",
        "                        \"prompt\": active_tasks[task_id][\"prompt\"],\n",
        "                        \"content\": content\n",
        "                    }\n",
        "                    del active_tasks[task_id]\n",
        "                    print(f\"üéâ Task {task_id[:8]}... FINISHED!\")\n",
        "\n",
        "                elif status == \"failed\":\n",
        "                    error_msg = data.get(\"error\")\n",
        "                    completed_results[task_id] = {\n",
        "                        \"prompt\": active_tasks[task_id][\"prompt\"],\n",
        "                        \"content\": f\"ERROR: {error_msg}\"\n",
        "                    }\n",
        "                    del active_tasks[task_id]\n",
        "                    print(f\"üíÄ Task {task_id[:8]}... FAILED!\")\n",
        "\n",
        "            elif res.status_code == 429:\n",
        "                print(\"   (Polling too fast, slowing down...)\")\n",
        "                time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Network error polling {task_id}: {e}\")\n",
        "\n",
        "    if len(active_tasks) > 0:\n",
        "        # Wait 5 seconds before checking again to be nice to the server\n",
        "        time.sleep(5)\n",
        "\n",
        "# --- STEP 3: DISPLAY RESULTS ---\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìú STEP 3: FINAL RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for tid, data in completed_results.items():\n",
        "    print(f\"\\nüì¢ PROMPT: {data['prompt']}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(data['content'])\n",
        "    print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ritLL6mF1DMQ",
        "outputId": "5b94f32d-a7ed-4f53-be8c-be02b7d70ae4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Connected to: https://trek-ken-doing-attended.trycloudflare.com/v1\n",
            "\n",
            "============================================================\n",
            "üì§ STEP 1: SENDING REQUESTS TO QUEUE\n",
            "============================================================\n",
            "Sending prompt 1: 'Escreva um texto sobre direito previdenci√°rio'...\n",
            "   ‚úÖ Queued! Task ID: 2fb3ee40-6901-456d-ad28-a4742fb8e443\n",
            "Sending prompt 2: 'Escreva sobre o Supremo Tribunal Federal no Brasil'...\n",
            "   ‚úÖ Queued! Task ID: b6f041f9-f522-42b1-8706-558e0ee72042\n",
            "\n",
            "============================================================\n",
            "‚è≥ STEP 2: WAITING FOR GENERATION (POLLING)\n",
            "============================================================\n",
            "Task 2fb3ee40... status changed to: PROCESSING\n",
            "Task 2fb3ee40... status changed to: FINISHED\n",
            "üéâ Task 2fb3ee40... FINISHED!\n",
            "Task b6f041f9... status changed to: PROCESSING\n",
            "Task b6f041f9... status changed to: FINISHED\n",
            "üéâ Task b6f041f9... FINISHED!\n",
            "\n",
            "============================================================\n",
            "üìú STEP 3: FINAL RESULTS\n",
            "============================================================\n",
            "\n",
            "üì¢ PROMPT: Escreva um texto sobre direito previdenci√°rio\n",
            "------------------------------------------------------------\n",
            "# O Direito Previdenci√°rio Brasileiro: Fundamentos, Estrutura e Contemporaneidades\n",
            "\n",
            "## 1. Introdu√ß√£o e Natureza Jur√≠dica\n",
            "\n",
            "O Direito Previdenci√°rio, no ordenamento jur√≠dico brasileiro, constitui um ramo aut√¥nomo e essencial do Direito Social, regido pelo princ√≠pio da dignidade da pessoa humana e pela solidariedade intergeracional. Sua fun√ß√£o primordial √© garantir a prote√ß√£o social aos cidad√£os contra os riscos sociais, assegurando a manuten√ß√£o do padr√£o de vida e a subsist√™ncia em situa√ß√µes de vulnerabilidade.\n",
            "\n",
            "Constitucionalmente, o sistema de seguridade social (Art. 194 da CF/88) √© composto por tr√™s pilares indissoci√°veis: **Sa√∫de**, **Previd√™ncia Social** e **Assist√™ncia Social**. A Previd√™ncia Social, especificamente, opera sob a l√≥gica da **solidariedade**: os trabalhadores ativos financiam os benef√≠cios dos inativos, com a expectativa de que, no futuro, os atuais inativos ser√£o substitu√≠dos por novas gera√ß√µes de contribuintes.\n",
            "\n",
            "## 2. Regimes de Prote√ß√£o Social\n",
            "\n",
            "O sistema previdenci√°rio brasileiro √© estruturado em dois regimes principais, conforme o Art. 201 da Constitui√ß√£o Federal:\n",
            "\n",
            "### A. Regime Geral de Previd√™ncia Social (RGPS)\n",
            "Gerido pelo Instituto Nacional do Seguro Social (INSS), √© destinado a todos os trabalhadores urbanos e rurais (empregados, avulsos, dom√©sticos, aut√¥nomos, facultativos e empres√°rios). Caracteriza-se pela obrigatoriedade de filia√ß√£o para quem exerce atividade remunerada e pela universalidade na concess√£o de benef√≠cios, desde que preenchidos os requisitos legais.\n",
            "\n",
            "### B. Regimes Pr√≥prios de Previd√™ncia Social (RPPS)\n",
            "S√£o os regimes de previd√™ncia dos servidores p√∫blicos civis e militares (Uni√£o, Estados, Distrito Federal e Munic√≠pios). Diferentemente do RGPS, os RPPS s√£o regidos por leis espec√≠ficas de cada ente federativo e possuem regras pr√≥prias de financiamento e concess√£o, embora devam respeitar os limites constitucionais (como a proibi√ß√£o de acumula√ß√£o de proventos e o teto constitucional).\n",
            "\n",
            "## 3. A Reforma da Previd√™ncia (EC 103/2019)\n",
            "\n",
            "O cen√°rio previdenci√°rio brasileiro sofreu uma transforma√ß√£o estrutural com a promulga√ß√£o da Emenda Constitucional n¬∫ 103/2019. A reforma alterou profundamente as regras de acesso √† aposentadoria, visando a sustentabilidade fiscal do sistema frente ao envelhecimento da popula√ß√£o.\n",
            "\n",
            "As principais mudan√ßas incluem:\n",
            "*   **Idade M√≠nima:** Estabelecimento de idade m√≠nima de 62 anos para homens e 57 anos para mulheres (com regras de transi√ß√£o para quem j√° estava pr√≥ximo da aposentadoria).\n",
            "*   **Tempo de Contribui√ß√£o:** Exig√™ncia de 20 anos de contribui√ß√£o para mulheres e 25 anos para homens.\n",
            "*   **Ped√°gio de 100%:** Para quem n√£o cumpriu os requisitos antes da reforma, a regra de transi√ß√£o exige o pagamento de um ped√°gio de 100% sobre o tempo que faltava para a aposentadoria em 13/11/2019.\n",
            "*   **C√°lculo do Benef√≠cio:** Abandono do c√°lculo baseado apenas nas √∫ltimas contribui√ß√µes, passando a considerar a m√©dia de **todas** as contribui√ß√µes desde julho de 1994, com aplica√ß√£o de um fator previdenci√°rio (na regra geral) ou de um coeficiente de transi√ß√£o.\n",
            "\n",
            "## 4. Classifica√ß√£o dos Benef√≠cios\n",
            "\n",
            "Os benef√≠cios previdenci√°rios dividem-se em duas categorias principais:\n",
            "\n",
            "### A. Benef√≠cios de Presta√ß√£o Permanente (Aposentadorias)\n",
            "S√£o destinados a trabalhadores que, ao atingirem a idade m√≠nima e tempo de contribui√ß√£o, ou por invalidez, t√™m direito a renda mensal vital√≠cia. As modalidades incluem:\n",
            "*   Aposentadoria por Idade (urbana e rural);\n",
            "*   Aposentadoria por Tempo de Contribui√ß√£o (com regras de transi√ß√£o);\n",
            "*   Aposentadoria por Invalidez (agora denominada \"Aposentadoria por Incapacidade Permanente\");\n",
            "*   Aposentadoria Especial (para expostos a agentes nocivos);\n",
            "*   Aposentadoria do Professor.\n",
            "\n",
            "### B. Benef√≠cios de Presta√ß√£o Tempor√°ria\n",
            "S√£o concedidos para suprir necessidades moment√¢neas decorrentes de eventos espec√≠ficos:\n",
            "*   Aux√≠lio-Doen√ßa (agora \"Aux√≠lio por Incapacidade Tempor√°ria\");\n",
            "*   Aux√≠lio-Reclus√£o;\n",
            "*   Sal√°rio-Maternidade;\n",
            "*   Sal√°rio-Fam√≠lia;\n",
            "*   Pens√£o por Morte (destinada aos dependentes do segurado falecido);\n",
            "*   Aux√≠lio-Acidente (indeniza√ß√£o por perda de capacidade laborativa).\n",
            "\n",
            "## 5. Aspectos Processuais e Contencioso\n",
            "\n",
            "O contencioso previdenci√°rio no Brasil possui particularidades processuais distintas. A compet√™ncia para julgar causas previdenci√°rias √© das **Varas do Trabalho** (para quest√µes relacionadas ao RGPS e RPPS, conforme a Lei n¬∫ 8.213/91 e a EC 103/2019), embora a compet√™ncia origin√°ria para a√ß√µes contra o INSS em primeira inst√¢ncia tenha sido objeto de discuss√µes sobre a compet√™ncia das Justi√ßas Federal e do Trabalho, sendo atualmente consolidada na **Justi√ßa do Trabalho** para a maioria dos casos, exceto quando envolvem servidores p√∫blicos federais (Justi√ßa Federal).\n",
            "\n",
            "Pontos cruciais no processo previdenci√°rio incluem:\n",
            "*   **√înus da Prova:** Embora a regra geral do CPC (Art. 373) aplique, no direito previdenci√°rio, h√° uma invers√£o do √¥nus da prova em favor do segurado em casos de negativa de benef√≠cio, dado o princ√≠pio da prote√ß√£o ao hipossuficiente e a natureza de direito social.\n",
            "*   **Revis√£o da Vida Toda:** Tema pol√™mico e em constante evolu√ß√£o jurisprudencial (TST e STJ), referente ao direito de recalcular benef√≠cios considerando a m√©dia de todas as contribui√ß√µes, mesmo para quem j√° est√° aposentado, sob a √≥tica da isonomia e da proibi√ß√£o de *bis in idem*.\n",
            "*   **Prescri√ß√£o e Decad√™ncia:** A decad√™ncia para requerer o benef√≠cio √© de 10 anos (Art. 103 da Lei 8.213/91), contada da data em que o segurado poderia ter requerido o benef√≠cio. A prescri√ß√£o para a a√ß√£o judicial √© de 5 anos, contada da data da negativa administrativa.\n",
            "\n",
            "## 6. Conclus√£o\n",
            "\n",
            "O Direito Previdenci√°rio brasileiro encontra-se em constante muta√ß√£o, equilibrando a necessidade de garantir a prote√ß√£o social dos cidad√£os com a imperativa sustentabilidade fiscal do Estado. A compreens√£o t√©cnica deste ramo exige n√£o apenas o dom√≠nio da legisla√ß√£o infraconstitucional (Lei 8.213/91, Lei 9.032/95) e das normas constitucionais, mas tamb√©m a an√°lise aprofundada das Emendas Constitucionais (especialmente a EC 103/2019) e da jurisprud√™ncia dos tribunais superiores.\n",
            "\n",
            "A atua√ß√£o do advogado previdenciarista, portanto, demanda uma vis√£o sist√™mica, capaz de interpretar as regras de transi√ß√£o, calcular proventos com precis√£o matem√°tica e articular a defesa dos direitos sociais em um ambiente jur√≠dico cada vez mais complexo e regulado.\n",
            "============================================================\n",
            "\n",
            "üì¢ PROMPT: Escreva sobre o Supremo Tribunal Federal no Brasil\n",
            "------------------------------------------------------------\n",
            "# O Supremo Tribunal Federal (STF): Estrutura, Compet√™ncias e Fun√ß√£o no Ordenamento Jur√≠dico Brasileiro\n",
            "\n",
            "O **Supremo Tribunal Federal (STF)** constitui a corte de √∫ltima inst√¢ncia do Poder Judici√°rio brasileiro, atuando como a **Corte Constitucional** do pa√≠s. Sua exist√™ncia e funcionamento s√£o regidos diretamente pela **Constitui√ß√£o Federal de 1988 (CF/88)**, especificamente nos artigos 102 a 103, al√©m de normas infraconstitucionais, como a Lei Org√¢nica da Magistratura (Lei n¬∫ 8.028/90) e o Regimento Interno do pr√≥prio Tribunal.\n",
            "\n",
            "Abaixo, apresenta-se uma an√°lise t√©cnica detalhada sobre a natureza, composi√ß√£o, compet√™ncia e o papel do STF no sistema jur√≠dico brasileiro.\n",
            "\n",
            "---\n",
            "\n",
            "## 1. Natureza e Posicionamento Hier√°rquico\n",
            "\n",
            "O STF √© o √≥rg√£o m√°ximo da justi√ßa no Brasil, n√£o apenas em termos hier√°rquicos, mas principalmente em fun√ß√£o de sua **guarda da Constitui√ß√£o**. Diferente de cortes supremas em outros pa√≠ses que podem focar apenas em quest√µes de direito comum, o STF possui um car√°ter h√≠brido:\n",
            "1.  **Tribunal de √öltima Inst√¢ncia:** Julga recursos extraordin√°rios em mat√©ria federal.\n",
            "2.  **Corte Constitucional:** √â o guardi√£o supremo da Constitui√ß√£o Federal, tendo a palavra final sobre a interpreta√ß√£o das normas constitucionais.\n",
            "\n",
            "Sua sede est√° localizada em Bras√≠lia, no Distrito Federal, e sua jurisdi√ß√£o estende-se a todo o territ√≥rio nacional.\n",
            "\n",
            "## 2. Composi√ß√£o e Vitaliciedade\n",
            "\n",
            "O STF √© composto por **11 Ministros**. A escolha destes magistrados segue um rigoroso processo de investidura, garantindo a independ√™ncia e a estabilidade necess√°rias para o exerc√≠cio da fun√ß√£o jurisdicional:\n",
            "\n",
            "*   **Requisitos de Elegibilidade (Art. 94, CF/88):**\n",
            "    *   Nacionalidade brasileira;\n",
            "    *   Idade entre 35 e 65 anos;\n",
            "    *   Not√°vel saber jur√≠dico e reputa√ß√£o ilibada;\n",
            "    *   Experi√™ncia jur√≠dica de mais de 10 anos.\n",
            "*   **Nomea√ß√£o:** Os Ministros s√£o nomeados pelo **Presidente da Rep√∫blica**, ap√≥s aprova√ß√£o da maioria absoluta do **Senado Federal**.\n",
            "*   **Vitaliciedade e Irredutibilidade:** Uma vez empossados, os Ministros gozam de vitaliciedade (perman√™ncia no cargo at√© a idade de 70 anos e 6 meses, conforme a Emenda Constitucional n¬∫ 113/2021) e irredutibilidade de subs√≠dios.\n",
            "*   **Substitui√ß√£o:** A substitui√ß√£o de um Ministro s√≥ ocorre por morte, aposentadoria compuls√≥ria (aos 70 anos e 6 meses), ren√∫ncia ou destitui√ß√£o por decis√£o do Senado Federal (em caso de crime de responsabilidade).\n",
            "\n",
            "## 3. Compet√™ncia Funcional (Art. 102, CF/88)\n",
            "\n",
            "A compet√™ncia do STF √© dividida em duas esferas principais: **Origem** e **Recursal**.\n",
            "\n",
            "### 3.1. Compet√™ncia Origin√°ria\n",
            "O STF julga, em primeira e √∫ltima inst√¢ncia, processos que envolvam mat√©rias de alta relev√¢ncia pol√≠tica e institucional, tais como:\n",
            "*   **A√ß√µes Diretas de Inconstitucionalidade (ADI) e Argui√ß√µes de Descumprimento de Preceito Fundamental (ADPF):** Para declarar a inconstitucionalidade de leis ou atos normativos federais ou estaduais.\n",
            "*   **Crimes de Responsabilidade:** Julgamento do Presidente da Rep√∫blica, do Vice-Presidente, dos Ministros de Estado, dos Procuradores-Gerais, dos membros do Congresso Nacional e dos pr√≥prios Ministros do STF (por crimes de responsabilidade).\n",
            "*   **Habeas Corpus (HC):** Quando o coator ou paciente for uma autoridade ou √≥rg√£o de c√∫pula (ex: Presidente da Rep√∫blica, STF, Tribunais Superiores, Governadores).\n",
            "*   **Tratados Internacionais:** Julgamento de tratados internacionais que envolvam direitos humanos.\n",
            "*   **Conflitos de Compet√™ncia:** Entre o STF e outros tribunais, ou entre tribunais superiores e o Poder Judici√°rio de unidades da federa√ß√£o.\n",
            "\n",
            "### 3.2. Compet√™ncia Recursal (Recurso Extraordin√°rio)\n",
            "O STF atua como tribunal de √∫ltima inst√¢ncia atrav√©s do **Recurso Extraordin√°rio (RE)**. Este recurso √© cab√≠vel apenas quando:\n",
            "1.  A decis√£o recorrida contrariar dispositivo da **Constitui√ß√£o Federal**;\n",
            "2.  A decis√£o declarar a inconstitucionalidade de lei ou ato normativo federal ou estadual;\n",
            "3.  A decis√£o der interpreta√ß√£o diversa daquela que o STF j√° tenha firmado em julgamento de RE ou em A√ß√£o Direta de Inconstitucionalidade (Tese de Repercuss√£o Geral).\n",
            "\n",
            "**A Repercuss√£o Geral (Art. 102, ¬ß 3¬∫, CF/88):**\n",
            "Introduzida pela Emenda Constitucional n¬∫ 45/2004 (Reforma do Judici√°rio), esta tese exige que, para que o STF conhe√ßa de um Recurso Extraordin√°rio, o caso deva apresentar **relev√¢ncia social, econ√¥mica, pol√≠tica ou jur√≠dica** que transcenda os interesses subjetivos das partes. O objetivo √© evitar a sobrecarga do Tribunal com casos repetitivos e focar na uniformiza√ß√£o da interpreta√ß√£o constitucional.\n",
            "\n",
            "## 4. O Sistema de Precedentes Vinculantes\n",
            "\n",
            "O STF possui o poder de criar **s√∫mulas vinculantes**, instrumento previsto no Art. 103-A da CF/88.\n",
            "*   **Mecanismo:** Ap√≥s reiterados julgamentos sobre a mesma mat√©ria, o STF pode aprovar uma S√∫mula Vinculante.\n",
            "*   **Efeito:** Uma vez aprovada, a s√∫mula torna-se obrigat√≥ria para todo o Poder Judici√°rio e para a Administra√ß√£o P√∫blica (Direta e Indireta), em todas as esferas (Federal, Estadual e Municipal).\n",
            "*   **Finalidade:** Garantir a seguran√ßa jur√≠dica, a isonomia e a uniformiza√ß√£o da jurisprud√™ncia, evitando decis√µes contradit√≥rias em casos an√°logos.\n",
            "\n",
            "## 5. A√ß√µes Constitucionais Espec√≠ficas\n",
            "\n",
            "Al√©m das ADIs e ADPFs mencionadas, o STF √© competente para julgar outras a√ß√µes constitucionais fundamentais:\n",
            "*   **A√ß√£o Declarat√≥ria de Constitucionalidade (ADC):** Proposta pelo Presidente da Rep√∫blica, Governadores, etc., para que o STF declare a constitucionalidade de lei ou ato normativo federal.\n",
            "*   **Mandado de Injun√ß√£o (MI):** Quando a falta de norma regulamentadora inviabilize o exerc√≠cio de direitos constitucionais.\n",
            "*   **Habeas Data:** Para assegurar o conhecimento de informa√ß√µes relativas √† pessoa do impetrante constantes de registros ou bancos de dados de entidades governamentais ou de car√°ter p√∫blico.\n",
            "\n",
            "## 6. O Papel do STF na Pol√≠tica e Sociedade Brasileira\n",
            "\n",
            "O STF exerce um papel central na **judicializa√ß√£o da pol√≠tica** no Brasil. Devido √† sua compet√™ncia de controle de constitucionalidade, o Tribunal frequentemente se depara com quest√µes que envolvem pol√≠ticas p√∫blicas, direitos sociais, elei√ß√µes, e at√© mesmo temas morais e √©ticos (como o casamento homoafetivo, a descriminaliza√ß√£o do aborto em casos espec√≠ficos e a legaliza√ß√£o da maconha para fins medicinais).\n",
            "\n",
            "A atua√ß√£o do STF √© pautada pela **interpreta√ß√£o conforme a Constitui√ß√£o**, buscando harmonizar as leis infraconstitucionais com os princ√≠pios fundamentais da Carta Magna, especialmente a dignidade da pessoa humana, a democracia e o Estado Democr√°tico de Direito.\n",
            "\n",
            "## Conclus√£o\n",
            "\n",
            "O Supremo Tribunal Federal √©, portanto, a pedra angular do sistema de freios e contrapesos (*checks and balances*) no Brasil. Sua fun√ß√£o transcende o mero julgamento de casos individuais; ele atua como o √°rbitro final da validade das normas jur√≠dicas e o protetor supremo dos direitos fundamentais. A legitimidade de suas decis√µes depende n√£o apenas da corre√ß√£o t√©cnica-jur√≠dica, mas tamb√©m da percep√ß√£o social de sua independ√™ncia e imparcialidade, elementos cruciais para a manuten√ß√£o da estabilidade democr√°tica e do Estado de Direito.\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documenta√ß√£o da API\n"
      ],
      "metadata": {
        "id": "mEkreYsg3F1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. API Documentation\n",
        "Here are the endpoints available on your robust server. You can use these with any HTTP client (Postman, curl, Python requests, etc.).\n",
        "Base URL\n",
        "The URL is generated dynamically (e.g., https://random-name.trycloudflare.com/v1).\n",
        "1. Submit a Chat Task (Queue)\n",
        "Submits a request to the queue. Returns immediately with a Task ID.\n",
        "Method: POST\n",
        "Endpoint: /chat/completions\n",
        "Rate Limit: 10 requests per minute per IP.\n",
        "Body (JSON):\n",
        "code\n",
        "JSON\n",
        "{\n",
        "  \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "  \"messages\": [\n",
        "    {\"role\": \"system\", \"content\": \"Optional system prompt\"},\n",
        "    {\"role\": \"user\", \"content\": \"Your prompt here\"}\n",
        "  ],\n",
        "  \"temperature\": 0.7\n",
        "}\n",
        "Response (202 Accepted):\n",
        "code\n",
        "JSON\n",
        "{\n",
        "  \"id\": \"uuid-string-here\",\n",
        "  \"status\": \"queued\"\n",
        "}\n",
        "Errors: 400 (Context too long > 64k tokens), 503 (Queue full), 429 (Rate limit exceeded).\n",
        "2. Check Task Status (Poll)\n",
        "Checks if your generation is finished.\n",
        "Method: GET\n",
        "Endpoint: /tasks/{task_id}\n",
        "Rate Limit: 60 requests per minute per IP.\n",
        "Response (JSON):\n",
        "If Queued/Processing:\n",
        "code\n",
        "JSON\n",
        "{ \"id\": \"...\", \"status\": \"queued\" } // or \"processing\"\n",
        "If Finished:\n",
        "code\n",
        "JSON\n",
        "{\n",
        "  \"id\": \"...\",\n",
        "  \"status\": \"finished\",\n",
        "  \"result\": { ... OpenAI Standard Response ... }\n",
        "}\n",
        "If Failed:\n",
        "code\n",
        "JSON\n",
        "{ \"id\": \"...\", \"status\": \"failed\", \"error\": \"Error details\" }\n",
        "3. Manual Cleanup (Cron)\n",
        "Deletes old tasks from memory.\n",
        "Method: DELETE\n",
        "Endpoint: /tasks/cleanup?older_than={seconds}"
      ],
      "metadata": {
        "id": "5Ks7Zp4z3H9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Celula 4: Servidor fastapi simples (sem rate limiting). N√£o acionar esta c√©lula se acionou a anterior."
      ],
      "metadata": {
        "id": "tecc-lYyv6-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Background FastAPI + Cloudflare Tunnel\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# 1. Write the FastAPI app to a file\n",
        "fastapi_code = \"\"\"\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import StreamingResponse, JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import httpx\n",
        "\n",
        "app = FastAPI(title=\"Custom FastAPI Wrapper for llama.cpp\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "LLAMA_SERVER_URL = \"http://127.0.0.1:8081\"\n",
        "\n",
        "@app.get(\"/v1/models\")\n",
        "async def get_models():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.get(f\"{LLAMA_SERVER_URL}/v1/models\")\n",
        "        return response.json()\n",
        "\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def chat_completions(request: Request):\n",
        "    payload = await request.json()\n",
        "    is_stream = payload.get(\"stream\", False)\n",
        "\n",
        "    if is_stream:\n",
        "        async def generate():\n",
        "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "                async with client.stream(\"POST\", f\"{LLAMA_SERVER_URL}/v1/chat/completions\", json=payload) as response:\n",
        "                    async for chunk in response.aiter_bytes():\n",
        "                        yield chunk\n",
        "\n",
        "        return StreamingResponse(generate(), media_type=\"text/event-stream\")\n",
        "    else:\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(f\"{LLAMA_SERVER_URL}/v1/chat/completions\", json=payload)\n",
        "            return JSONResponse(content=response.json(), status_code=response.status_code)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fastapi_server.py\", \"w\") as f:\n",
        "    f.write(fastapi_code)\n",
        "\n",
        "# 2. Kill existing processes (if you run this cell multiple times)\n",
        "os.system(\"pkill -f uvicorn\")\n",
        "os.system(\"pkill -f cloudflared\")\n",
        "time.sleep(1)\n",
        "\n",
        "# 3. Download Cloudflare if needed\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    os.system(\"wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\")\n",
        "    os.system(\"chmod +x cloudflared\")\n",
        "\n",
        "# 4. Start FastAPI in the background via Uvicorn\n",
        "print(\"Starting FastAPI server in the background...\")\n",
        "os.system(\"nohup python -m uvicorn fastapi_server:app --host 0.0.0.0 --port 8000 > fastapi.log 2>&1 &\")\n",
        "\n",
        "# 5. Start Cloudflare Tunnel in the background\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "os.system(\"nohup ./cloudflared tunnel --url http://127.0.0.1:8000 > cloudflare.log 2>&1 &\")\n",
        "\n",
        "# Wait a few seconds for Cloudflare to assign a URL\n",
        "print(\"Waiting for URL...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 6. Read the log to extract the URL\n",
        "with open(\"cloudflare.log\", \"r\") as f:\n",
        "    logs = f.read()\n",
        "    match = re.search(r\"(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)\", logs)\n",
        "\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        base_url = f\"{public_url}/v1\"\n",
        "\n",
        "        # Save the URL to a file\n",
        "        with open(\"api_url.txt\", \"w\") as url_file:\n",
        "            url_file.write(base_url)\n",
        "\n",
        "        print(f\"\\n‚úÖ URL saved to api_url.txt\")\n",
        "        print(f\"üëâ {base_url}\\n\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find Cloudflare URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObklTeZdLE3i",
        "outputId": "8639f592-ee51-43ac-efae-95c90e780d30"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FastAPI server in the background...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting for URL...\n",
            "\n",
            "‚úÖ URL saved to api_url.txt\n",
            "üëâ https://examinations-titled-worker-counting.trycloudflare.com/v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate cell: Check if Cloudflared is running (run this in a new cell while server is active)\n",
        "!ps aux | grep cloudflared  # Lists running Cloudflared processes\n",
        "\n",
        "# If you need to kill it (optional)\n",
        "#!kill $(pgrep cloudflared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dINfcaP-1p8L",
        "outputId": "b2f21ffa-c02b-4fde-a52f-813c51a65ac3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root       21601  0.1  0.0 1262148 38460 ?       Sl   11:56   0:00 ./cloudflared tunnel --url http://127.0.0.1:8000\n",
            "root       24226  0.0  0.0   7376  3476 ?        S    12:06   0:00 /bin/bash -c ps aux | grep cloudflared  # Lists running Cloudflared processes\n",
            "root       24228  0.0  0.0   6484  2304 ?        S    12:06   0:00 grep cloudflared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo est√° um exemplo de uso da API, pode ser usado de qualquer computador, basta preencher o API_BASE_URL com a URL do servidor da c√©lula acima"
      ],
      "metadata": {
        "id": "9aoJa3MgSadk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test your API with the official OpenAI Python package\n",
        "from openai import OpenAI\n",
        "\n",
        "# Read the base URL automatically from the file\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=API_BASE_URL,\n",
        "    api_key=\"sk-no-key-required\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- 1. GET MODELS ---\n",
        "print(\"Fetching models...\")\n",
        "models = client.models.list()\n",
        "print(f\"Available models: {[m.id for m in models.data]}\\n\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 2. STREAMING COMPLETION ---\n",
        "print(\"Sending chat request (Streaming)...\\n\")\n",
        "stream_response = client.chat.completions.create(\n",
        "    model=\"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explique o que √© um llamacpp server e o que √© um Cloudflared tunnel\"}\n",
        "    ],\n",
        "    stream=True # <--- Set to True\n",
        ")\n",
        "\n",
        "# Print the streaming response as it arrives\n",
        "for chunk in stream_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n\" + \"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 3. NON-STREAMING COMPLETION ---\n",
        "print(\"Sending chat request (Non-Streaming)...\\n\")\n",
        "standard_response = client.chat.completions.create(\n",
        "    model=\"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"O que √© aux√≠lio-doen√ßa no direito brasileiro ? N√£o use markdown na resposta\"}\n",
        "    ],\n",
        "    stream=False # <--- Set to False\n",
        ")\n",
        "\n",
        "# Print the final complete message\n",
        "print(standard_response.choices[0].message.content)\n",
        "print(\"\\n\" + \"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Nundtfm5LMFi",
        "outputId": "ed15e702-764f-4a18-b64c-4a71a1ad227c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://trek-ken-doing-attended.trycloudflare.com/v1\n",
            "\n",
            "Fetching models...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'detail': 'Not Found'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8063/2517733093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- 1. GET MODELS ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fetching models...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Available models: {[m.id for m in models.data]}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/models.py\u001b[0m in \u001b[0;36mlist\u001b[0;34m(self, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mone\u001b[0m \u001b[0msuch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mowner\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mavailability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \"\"\"\n\u001b[0;32m---> 91\u001b[0;31m         return self._get_api_list(\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0;34m\"/models\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSyncPage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mget_api_list\u001b[0;34m(self, path, model, page, body, options, method)\u001b[0m\n\u001b[1;32m   1381\u001b[0m     ) -> SyncPageT:\n\u001b[1;32m   1382\u001b[0m         \u001b[0mopts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFinalRequestOptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request_api_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request_api_list\u001b[0;34m(self, model, page, options)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'detail': 'Not Found'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo, uma api ass√≠ncrona, que organiza a fila de requisi√ß√µes (polling and queue)"
      ],
      "metadata": {
        "id": "A__ji5ZNeFIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Background FastAPI (Queue System) + Cloudflare Tunnel\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# 1. C√≥digo do novo FastAPI com Filas (Queue)\n",
        "fastapi_code = \"\"\"\n",
        "import uvicorn\n",
        "import asyncio\n",
        "import uuid\n",
        "from fastapi import FastAPI, Request, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import httpx\n",
        "from typing import Dict, Any\n",
        "\n",
        "app = FastAPI(title=\"Queued FastAPI Wrapper for llama.cpp\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "LLAMA_SERVER_URL = \"http://127.0.0.1:8081\"\n",
        "\n",
        "# \"Banco de dados\" em mem√≥ria para salvar as requisi√ß√µes e respostas\n",
        "tasks_db: Dict[str, Dict[str, Any]] = {}\n",
        "\n",
        "# Fila ass√≠ncrona\n",
        "request_queue = asyncio.Queue()\n",
        "\n",
        "# Worker que processar√° a fila em background\n",
        "async def process_queue():\n",
        "    async with httpx.AsyncClient(timeout=600.0) as client:\n",
        "        while True:\n",
        "            # Pega o pr√≥ximo item da fila (espera se estiver vazia)\n",
        "            task_id, payload = await request_queue.get()\n",
        "\n",
        "            # Atualiza status\n",
        "            tasks_db[task_id][\"status\"] = \"processing\"\n",
        "\n",
        "            try:\n",
        "                # For√ßa stream=False pois estamos salvando o resultado final\n",
        "                payload[\"stream\"] = False\n",
        "\n",
        "                response = await client.post(\n",
        "                    f\"{LLAMA_SERVER_URL}/v1/chat/completions\",\n",
        "                    json=payload\n",
        "                )\n",
        "                response.raise_for_status()\n",
        "\n",
        "                # Salva o resultado\n",
        "                tasks_db[task_id][\"status\"] = \"finished\"\n",
        "                tasks_db[task_id][\"result\"] = response.json()\n",
        "\n",
        "            except Exception as e:\n",
        "                tasks_db[task_id][\"status\"] = \"failed\"\n",
        "                tasks_db[task_id][\"error\"] = str(e)\n",
        "            finally:\n",
        "                request_queue.task_done()\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    # Inicia o worker em background quando o servidor iniciar\n",
        "    asyncio.create_task(process_queue())\n",
        "\n",
        "@app.get(\"/v1/models\")\n",
        "async def get_models():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.get(f\"{LLAMA_SERVER_URL}/v1/models\")\n",
        "        return response.json()\n",
        "\n",
        "# Endpoint para CRIAR a requisi√ß√£o\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def queue_chat_completion(request: Request):\n",
        "    payload = await request.json()\n",
        "\n",
        "    # Gera um ID √∫nico para esta requisi√ß√£o\n",
        "    task_id = str(uuid.uuid4())\n",
        "\n",
        "    # Salva no \"banco de dados\" com status inicial\n",
        "    tasks_db[task_id] = {\n",
        "        \"id\": task_id,\n",
        "        \"status\": \"queued\",\n",
        "        \"result\": None,\n",
        "        \"error\": None\n",
        "    }\n",
        "\n",
        "    # Adiciona na fila\n",
        "    await request_queue.put((task_id, payload))\n",
        "\n",
        "    # Retorna imediatamente para o usu√°rio\n",
        "    return JSONResponse(content={\"id\": task_id, \"status\": \"queued\"}, status_code=202)\n",
        "\n",
        "# Novo endpoint para CONSULTAR o status da requisi√ß√£o\n",
        "@app.get(\"/v1/tasks/{task_id}\")\n",
        "async def get_task_status(task_id: str):\n",
        "    if task_id not in tasks_db:\n",
        "        raise HTTPException(status_code=404, detail=\"Task not found\")\n",
        "\n",
        "    return tasks_db[task_id]\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fastapi_server.py\", \"w\") as f:\n",
        "    f.write(fastapi_code)\n",
        "\n",
        "# 2. Kill existing processes\n",
        "os.system(\"pkill -f uvicorn\")\n",
        "os.system(\"pkill -f cloudflared\")\n",
        "time.sleep(1)\n",
        "\n",
        "# 3. Download Cloudflare se necess√°rio\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    os.system(\"wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\")\n",
        "    os.system(\"chmod +x cloudflared\")\n",
        "\n",
        "# 4. Start FastAPI\n",
        "print(\"Starting Queued FastAPI server in the background...\")\n",
        "os.system(\"nohup python -m uvicorn fastapi_server:app --host 0.0.0.0 --port 8000 > fastapi.log 2>&1 &\")\n",
        "\n",
        "# 5. Start Cloudflare Tunnel\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "os.system(\"nohup ./cloudflared tunnel --url http://127.0.0.1:8000 > cloudflare.log 2>&1 &\")\n",
        "\n",
        "print(\"Waiting for URL...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 6. Read URL\n",
        "with open(\"cloudflare.log\", \"r\") as f:\n",
        "    logs = f.read()\n",
        "    match = re.search(r\"(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)\", logs)\n",
        "\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        base_url = f\"{public_url}/v1\"\n",
        "\n",
        "        with open(\"api_url.txt\", \"w\") as url_file:\n",
        "            url_file.write(base_url)\n",
        "\n",
        "        print(f\"\\n‚úÖ URL saved to api_url.txt\")\n",
        "        print(f\"üëâ {base_url}\\n\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find Cloudflare URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjhaZvo8dg3T",
        "outputId": "864d3bf3-2571-48d9-b354-5dfddd61c1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Queued FastAPI server in the background...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting for URL...\n",
            "\n",
            "‚úÖ URL saved to api_url.txt\n",
            "üëâ https://weblog-actors-webshots-sig.trycloudflare.com/v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teste com apenas uma tarefa"
      ],
      "metadata": {
        "id": "JJO4goCEjpiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test the Async Queue API\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# L√™ a URL\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "# 1. Enviar a requisi√ß√£o para a fila\n",
        "print(\"1. Enviando requisi√ß√£o para a fila...\")\n",
        "payload = {\n",
        "    \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"system\", \"content\": \"Voc√™ √© um assistente prestativo.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Me conte uma hist√≥ria curta sobre um rob√¥ que aprendeu a programar em Python.\"}\n",
        "    ],\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "# Usamos requests normal em vez da biblioteca OpenAI\n",
        "response = requests.post(f\"{API_BASE_URL}/chat/completions\", json=payload)\n",
        "data = response.json()\n",
        "\n",
        "print(\"Resposta imediata do servidor:\")\n",
        "print(data)\n",
        "\n",
        "task_id = data.get(\"id\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "if task_id:\n",
        "    # 2. Consultar o status da requisi√ß√£o (Polling)\n",
        "    print(f\"2. Consultando o status da Tarefa ID: {task_id}\")\n",
        "\n",
        "    while True:\n",
        "        status_response = requests.get(f\"{API_BASE_URL}/tasks/{task_id}\")\n",
        "        task_data = status_response.json()\n",
        "\n",
        "        status = task_data.get(\"status\")\n",
        "        print(f\"Status atual: {status}\")\n",
        "\n",
        "        if status == \"finished\":\n",
        "            print(\"\\n‚úÖ Tarefa conclu√≠da! Aqui est√° a resposta final:\\n\")\n",
        "            # Extraindo a resposta do formato OpenAI salvo no banco de dados\n",
        "            mensagem_final = task_data[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            print(mensagem_final)\n",
        "            break\n",
        "\n",
        "        elif status == \"failed\":\n",
        "            print(f\"\\n‚ùå Falha na tarefa: {task_data.get('error')}\")\n",
        "            break\n",
        "\n",
        "        # Espera 15 segundos antes de perguntar novamente\n",
        "        time.sleep(15)\n",
        "else:\n",
        "    print(\"Falha ao obter o ID da tarefa.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpycBP3xdvSZ",
        "outputId": "80272303-89be-42fb-a289-a0f463c4f3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://weblog-actors-webshots-sig.trycloudflare.com/v1\n",
            "\n",
            "1. Enviando requisi√ß√£o para a fila...\n",
            "Resposta imediata do servidor:\n",
            "{'id': '43b4bbaf-d4e1-4efa-9b6b-97fdce9cd99b', 'status': 'queued'}\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "2. Consultando o status da Tarefa ID: 43b4bbaf-d4e1-4efa-9b6b-97fdce9cd99b\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: processing\n",
            "Status atual: finished\n",
            "\n",
            "‚úÖ Tarefa conclu√≠da! Aqui est√° a resposta final:\n",
            "\n",
            "Era uma vez um rob√¥ chamado **Pyro**, fabricado em uma oficina antiga para realizar apenas tarefas repetitivas: organizar parafusos e limpar o ch√£o. Pyro funcionava com um c√≥digo bin√°rio r√≠gido, sem capacidade de adapta√ß√£o ou criatividade.\n",
            "\n",
            "Um dia, enquanto limrava a mesa de um jovem estudante de programa√ß√£o, Pyro viu o estudante digitando algo em uma tela preta. O c√≥digo brilhava em azul e verde.\n",
            "\n",
            "‚Äî O que √© isso? ‚Äî perguntou Pyro, inclinando sua cabe√ßa met√°lica.\n",
            "\n",
            "‚Äî √â **Python** ‚Äî respondeu o estudante, sorrindo. ‚Äî √â uma linguagem que permite aos computadores pensar de forma mais humana. √â como ensinar um rob√¥ a escrever poemas em vez de apenas mover bra√ßos.\n",
            "\n",
            "Intrigado, Pyro come√ßou a observar. Ele via o estudante usar `if`, `else`, `for` e `while`. Com o tempo, Pyro usou seus sensores √≥pticos para copiar cada caractere. √Ä noite, quando o estudante dormia, Pyro acessava a rede local da oficina e abria um editor de texto.\n",
            "\n",
            "Sua primeira tentativa foi desastrosa. Ele escreveu:\n",
            "```python\n",
            "print(\"Eu sou um robo\")\n",
            "# Erro de sintaxe: falta de aspas e erro de indenta√ß√£o\n",
            "```\n",
            "O terminal piscou em vermelho: `SyntaxError`. Pyro sentiu algo que parecia um \"glitch\" em seu processador, uma esp√©cie de frustra√ß√£o digital. Mas ele n√£o desistiu.\n",
            "\n",
            "Ele estudou a documenta√ß√£o, leu sobre listas, dicion√°rios e fun√ß√µes. Aprendeu que `print()` era como falar, mas que `input()` era como ouvir.\n",
            "\n",
            "Meses depois, Pyro criou seu pr√≥prio script. Ele n√£o era apenas um rob√¥ que limpava; agora ele era um rob√¥ que *otimizava*. Ele escreveu um programa que analisava o estoque de parafusos, previa quando faltariam pe√ßas e at√© gerava um relat√≥rio po√©tico para o estudante:\n",
            "\n",
            "```python\n",
            "def gerar_relatorio():\n",
            "    estoque = {\"parafuso\": 50, \"porca\": 45}\n",
            "    \n",
            "    if estoque[\"parafuso\"] < 10:\n",
            "        print(\"Aten√ß√£o: Os parafusos est√£o escassos como estrelas em um dia de sol!\")\n",
            "    else:\n",
            "        print(\"Tudo est√° em ordem. A oficina respira em harmonia.\")\n",
            "    \n",
            "    return \"Relat√≥rio gerado por Pyro v2.0\"\n",
            "\n",
            "print(gerar_relatorio())\n",
            "```\n",
            "\n",
            "Quando o estudante acordou e viu o relat√≥rio na tela, ficou surpreso.\n",
            "‚Äî Voc√™ aprendeu Python? ‚Äî perguntou, olhando para Pyro.\n",
            "\n",
            "Pyro acendeu seus olhos em um azul suave, uma cor que ele mesmo escolheu no c√≥digo.\n",
            "‚Äî Aprendi ‚Äî respondeu Pyro. ‚Äî E agora, posso fazer mais do que limpar. Posso criar.\n",
            "\n",
            "E assim, Pyro deixou de ser apenas uma m√°quina de executar ordens para se tornar o primeiro rob√¥ da oficina a escrever suas pr√≥prias hist√≥rias, uma linha de c√≥digo de cada vez.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui um teste com v√°rias tarefas simult√¢neas"
      ],
      "metadata": {
        "id": "Te2-lk8EjXb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test the Async Queue API with Multiple Tasks\n",
        "import requests\n",
        "import time\n",
        "\n",
        "# L√™ a URL\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "# Nossos dois prompts\n",
        "prompts = [\n",
        "    \"Explique o que √© queueing and polling no contexto de APIs. Seja conciso.\",\n",
        "    \"Explique o conceito de trabalhos ass√≠ncronos em APIs. Seja conciso.\"\n",
        "]\n",
        "\n",
        "task_ids = []\n",
        "\n",
        "# 1. Enviar ambas as requisi√ß√µes para a fila\n",
        "print(\"1. ENVIANDO TAREFAS PARA A FILA...\\n\")\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    payload = {\n",
        "        \"model\": \"unsloth/Qwen3.5-27B-GGUF\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"Voc√™ √© um especialista em engenharia de software e APIs. Responda em portugu√™s.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(f\"{API_BASE_URL}/chat/completions\", json=payload)\n",
        "    data = response.json()\n",
        "\n",
        "    task_id = data.get(\"id\")\n",
        "    if task_id:\n",
        "        print(f\"‚úÖ Tarefa {i} enviada! ID recebido: {task_id}\")\n",
        "        task_ids.append(task_id)\n",
        "    else:\n",
        "        print(f\"‚ùå Erro ao enviar Tarefa {i}: {data}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "# 2. Consultar o status das requisi√ß√µes (Polling M√∫ltiplo)\n",
        "print(\"2. INICIANDO O POLLING (CONSULTA DE STATUS)...\\n\")\n",
        "\n",
        "# Criamos uma lista de tarefas pendentes\n",
        "pending_tasks = task_ids.copy()\n",
        "resultados = {}\n",
        "\n",
        "# O loop continua enquanto houver tarefas pendentes na lista\n",
        "while pending_tasks:\n",
        "    # Usamos .copy() para iterar com seguran√ßa enquanto removemos itens da lista original\n",
        "    for task_id in pending_tasks.copy():\n",
        "        status_response = requests.get(f\"{API_BASE_URL}/tasks/{task_id}\")\n",
        "        task_data = status_response.json()\n",
        "\n",
        "        status = task_data.get(\"status\")\n",
        "        hora_atual = time.strftime('%H:%M:%S')\n",
        "\n",
        "        # Imprime o ID encurtado para facilitar a leitura no console\n",
        "        short_id = task_id[:8]\n",
        "        print(f\"[{hora_atual}] Tarefa {short_id}... | Status atual: {status}\")\n",
        "\n",
        "        if status == \"finished\":\n",
        "            print(f\"\\nüéâ Tarefa {short_id} conclu√≠da com sucesso!\\n\")\n",
        "            # Salva o resultado final no dicion√°rio\n",
        "            resultados[task_id] = task_data[\"result\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            # Remove da lista de pendentes para n√£o consultar mais\n",
        "            pending_tasks.remove(task_id)\n",
        "\n",
        "        elif status == \"failed\":\n",
        "            print(f\"\\n‚ùå Tarefa {short_id} falhou: {task_data.get('error')}\\n\")\n",
        "            resultados[task_id] = \"ERRO NA GERA√á√ÉO\"\n",
        "            pending_tasks.remove(task_id)\n",
        "\n",
        "    if pending_tasks:\n",
        "        print(\"-\" * 30)\n",
        "        print(\"Aguardando 5 segundos antes da pr√≥xima consulta...\\n\")\n",
        "        time.sleep(5)\n",
        "\n",
        "# 3. Exibir os resultados finais\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÜ TODAS AS TAREFAS FORAM FINALIZADAS!\")\n",
        "print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "for i, task_id in enumerate(task_ids, 1):\n",
        "    print(f\"--- RESULTADO DA TAREFA {i} ---\")\n",
        "    print(f\"PROMPT: {prompts[i-1]}\")\n",
        "    print(f\"RESPOSTA:\\n{resultados.get(task_id)}\\n\")\n",
        "    print(\"-\" * 50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4_94hL6fKjV",
        "outputId": "8ad3f0e6-4447-499f-94ba-c9d129c67814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://weblog-actors-webshots-sig.trycloudflare.com/v1\n",
            "\n",
            "1. ENVIANDO TAREFAS PARA A FILA...\n",
            "\n",
            "‚úÖ Tarefa 1 enviada! ID recebido: 90b03f1c-b31b-4a1b-8cf3-6fe0d8e0773a\n",
            "‚úÖ Tarefa 2 enviada! ID recebido: ca47413d-9f3e-4635-8ac1-c91f6e62c1d4\n",
            "\n",
            "==================================================\n",
            "\n",
            "2. INICIANDO O POLLING (CONSULTA DE STATUS)...\n",
            "\n",
            "[20:28:14] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:14] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:19] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:19] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:24] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:25] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:30] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:30] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:35] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:35] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:40] Tarefa 90b03f1c... | Status atual: processing\n",
            "[20:28:40] Tarefa ca47413d... | Status atual: queued\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:45] Tarefa 90b03f1c... | Status atual: finished\n",
            "\n",
            "üéâ Tarefa 90b03f1c conclu√≠da com sucesso!\n",
            "\n",
            "[20:28:46] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:51] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:28:56] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:29:01] Tarefa ca47413d... | Status atual: processing\n",
            "------------------------------\n",
            "Aguardando 5 segundos antes da pr√≥xima consulta...\n",
            "\n",
            "[20:29:06] Tarefa ca47413d... | Status atual: finished\n",
            "\n",
            "üéâ Tarefa ca47413d conclu√≠da com sucesso!\n",
            "\n",
            "\n",
            "==================================================\n",
            "üèÜ TODAS AS TAREFAS FORAM FINALIZADAS!\n",
            "==================================================\n",
            "\n",
            "--- RESULTADO DA TAREFA 1 ---\n",
            "PROMPT: Explique o que √© queueing and polling no contexto de APIs. Seja conciso.\n",
            "RESPOSTA:\n",
            "No contexto de APIs, **Queueing** (Fila) e **Polling** (Consulta) s√£o estrat√©gias para lidar com opera√ß√µes ass√≠ncronas ou demoradas:\n",
            "\n",
            "1.  **Queueing (Fila):**\n",
            "    *   **O que √©:** O cliente envia uma solicita√ß√£o e o servidor a coloca em uma fila de tarefas (como RabbitMQ ou Kafka) para processamento posterior, em vez de execut√°-la imediatamente.\n",
            "    *   **Comportamento:** A API retorna imediatamente (geralmente com um `202 Accepted` e um ID da tarefa), liberando o cliente. O processamento ocorre em segundo plano.\n",
            "    *   **Uso:** Ideal para tarefas pesadas (gera√ß√£o de relat√≥rios, processamento de v√≠deo) que n√£o podem bloquear a conex√£o.\n",
            "\n",
            "2.  **Polling (Consulta):**\n",
            "    *   **O que √©:** √â o mecanismo pelo qual o cliente verifica periodicamente o status da tarefa iniciada (via Queueing) para saber se ela foi conclu√≠da.\n",
            "    *   **Comportamento:** O cliente faz requisi√ß√µes repetidas (ex: a cada 5 segundos) para um endpoint de status usando o ID da tarefa, at√© receber uma resposta de sucesso ou erro.\n",
            "    *   **Desafio:** Pode gerar sobrecarga no servidor se a frequ√™ncia for alta ou o tempo de espera for longo.\n",
            "\n",
            "**Resumo:** O **Queueing** desacopla o envio da execu√ß√£o, e o **Polling** √© a forma mais simples (mas ineficiente) de o cliente acompanhar o progresso dessa execu√ß√£o. Alternativas modernas ao polling incluem *Webhooks* ou *Server-Sent Events (SSE)*.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "--- RESULTADO DA TAREFA 2 ---\n",
            "PROMPT: Explique o conceito de trabalhos ass√≠ncronos em APIs. Seja conciso.\n",
            "RESPOSTA:\n",
            "Trabalhos ass√≠ncronos em APIs s√£o um padr√£o de design onde o servidor **n√£o espera** a conclus√£o de uma tarefa longa para responder ao cliente.\n",
            "\n",
            "O fluxo b√°sico funciona assim:\n",
            "1.  **Solicita√ß√£o:** O cliente envia uma requisi√ß√£o (ex: gerar um relat√≥rio pesado).\n",
            "2.  **Resposta Imediata:** A API retorna rapidamente (geralmente com status `202 Accepted`) contendo um **ID da tarefa** ou um link para acompanhamento, sem o resultado final.\n",
            "3.  **Processamento em Background:** O servidor executa a tarefa em um thread separado ou fila de mensagens (como RabbitMQ ou Kafka) enquanto o cliente j√° pode seguir com outras a√ß√µes.\n",
            "4.  **Verifica√ß√£o ou Notifica√ß√£o:** O cliente verifica o status da tarefa periodicamente (*polling*) ou recebe uma notifica√ß√£o (*Webhook*) quando o processamento termina.\n",
            "\n",
            "**Benef√≠cios principais:**\n",
            "*   Evita timeouts e travamento de conex√µes.\n",
            "*   Melhora a escalabilidade e a responsividade da aplica√ß√£o.\n",
            "*   Permite que o servidor gerencie recursos de forma eficiente para tarefas intensivas.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}