{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNtRxmLesWshnXuaHU3HzgZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fabiopauli/Qwen3.5-colab/blob/main/Server_Qwen27B_llamacpp_256k_context_L4_20gb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Celula abaixo demora 8 minutos para ser conclu√≠da"
      ],
      "metadata": {
        "id": "E564goWkNeir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Build llama.cpp with CUDA and run Qwen3.5-27B (non-thinking mode)\n",
        "!apt-get update -qq && apt-get install -qq -y pciutils build-essential cmake curl libcurl4-openssl-dev > /dev/null 2>&1\n",
        "\n",
        "!git clone --depth 1 https://github.com/ggml-org/llama.cpp 2>/dev/null || echo \"already cloned\"\n",
        "\n",
        "!cmake llama.cpp -B llama.cpp/build \\\n",
        "    -DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON > /dev/null 2>&1\n",
        "\n",
        "!cmake --build llama.cpp/build --config Release -j$(nproc) --clean-first --target llama-cli llama-server 2>&1 | tail -5\n",
        "\n",
        "!cp llama.cpp/build/bin/llama-* llama.cpp/\n",
        "\n",
        "# Download the model\n",
        "!pip install -q huggingface_hub hf_transfer\n",
        "!HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --local-dir unsloth/Qwen3.5-27B-GGUF \\\n",
        "    --include \"*UD-Q4_K_XL*\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAd6icy52eED",
        "outputId": "17dd0d1f-767f-4932-ec8d-f4e54ab0b764"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "already cloned\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\n",
            "[ 98%] Building CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\n",
            "[100%] Linking CXX executable ../../bin/llama-server\n",
            "[100%] Built target llama-server\n",
            "/bin/bash: line 1: huggingface-cli: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A c√©lula abaixo cria o servidor Llamacpp em background."
      ],
      "metadata": {
        "id": "nwbeV8MTNn0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Run llama-server in the background\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "\n",
        "# Kill any existing server to free up the port\n",
        "os.system(\"pkill -f llama-server\")\n",
        "time.sleep(2)\n",
        "\n",
        "os.environ[\"LLAMA_CACHE\"] = \"unsloth/Qwen3.5-27B-GGUF\"\n",
        "\n",
        "# Start the server using nohup so it runs in the background\n",
        "server_cmd = \"\"\"\n",
        "nohup ./llama.cpp/llama-server \\\n",
        "    -hf unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL \\\n",
        "    --host 127.0.0.1 \\\n",
        "    --port 8081 \\\n",
        "    --ctx-size 16384 \\\n",
        "    -ngl 99 \\\n",
        "    --temp 0.7 \\\n",
        "    --top-p 0.8 \\\n",
        "    --top-k 20 \\\n",
        "    --min-p 0.00 \\\n",
        "    --chat-template-kwargs '{\"enable_thinking\": false}' \\\n",
        "    --cache-type-k q8_0 \\\n",
        "    --cache-type-v q8_0 > llama_server.log 2>&1 &\n",
        "\"\"\"\n",
        "\n",
        "print(\"Starting llama-server on port 8081...\")\n",
        "os.system(server_cmd)\n",
        "\n",
        "# Wait for the server to spin up and load the model into VRAM\n",
        "print(\"Waiting for model to load into VRAM (this takes 30-60 seconds)...\")\n",
        "for i in range(600):\n",
        "    try:\n",
        "        import requests\n",
        "        res = requests.get(\"http://127.0.0.1:8081/health\")\n",
        "        if res.status_code == 200:\n",
        "            print(\"\\n‚úÖ llama-server is ready and listening on port 8081!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(2)\n",
        "    print(\".\", end=\"\", flush=True)\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è Server might not have started correctly. Check llama_server.log:\")\n",
        "    os.system(\"tail -n 20 llama_server.log\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rp-w75DFtw1",
        "outputId": "84e40366-677a-4e81-899c-fa0d09b8cef1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting llama-server on port 8081...\n",
            "Waiting for model to load into VRAM (this takes 30-60 seconds)...\n",
            ".......\n",
            "‚úÖ llama-server is ready and listening on port 8081!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A seguir, criamos outro servidor para gerar os endpoints da API, tamb√©m em background"
      ],
      "metadata": {
        "id": "kmwz2laJSJfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install dependencies for FastAPI wrapper\n",
        "!pip install -q fastapi uvicorn pyngrok httpx pydantic nest-asyncio"
      ],
      "metadata": {
        "id": "gxYG3JoJ0yU2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Background FastAPI + Cloudflare Tunnel\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "# 1. Write the FastAPI app to a file\n",
        "fastapi_code = \"\"\"\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.responses import StreamingResponse, JSONResponse\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import httpx\n",
        "\n",
        "app = FastAPI(title=\"Custom FastAPI Wrapper for llama.cpp\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "LLAMA_SERVER_URL = \"http://127.0.0.1:8081\"\n",
        "\n",
        "@app.get(\"/v1/models\")\n",
        "async def get_models():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.get(f\"{LLAMA_SERVER_URL}/v1/models\")\n",
        "        return response.json()\n",
        "\n",
        "@app.post(\"/v1/chat/completions\")\n",
        "async def chat_completions(request: Request):\n",
        "    payload = await request.json()\n",
        "    is_stream = payload.get(\"stream\", False)\n",
        "\n",
        "    if is_stream:\n",
        "        async def generate():\n",
        "            async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "                async with client.stream(\"POST\", f\"{LLAMA_SERVER_URL}/v1/chat/completions\", json=payload) as response:\n",
        "                    async for chunk in response.aiter_bytes():\n",
        "                        yield chunk\n",
        "\n",
        "        return StreamingResponse(generate(), media_type=\"text/event-stream\")\n",
        "    else:\n",
        "        async with httpx.AsyncClient(timeout=300.0) as client:\n",
        "            response = await client.post(f\"{LLAMA_SERVER_URL}/v1/chat/completions\", json=payload)\n",
        "            return JSONResponse(content=response.json(), status_code=response.status_code)\n",
        "\"\"\"\n",
        "\n",
        "with open(\"fastapi_server.py\", \"w\") as f:\n",
        "    f.write(fastapi_code)\n",
        "\n",
        "# 2. Kill existing processes (if you run this cell multiple times)\n",
        "os.system(\"pkill -f uvicorn\")\n",
        "os.system(\"pkill -f cloudflared\")\n",
        "time.sleep(1)\n",
        "\n",
        "# 3. Download Cloudflare if needed\n",
        "if not os.path.exists(\"cloudflared\"):\n",
        "    os.system(\"wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\")\n",
        "    os.system(\"chmod +x cloudflared\")\n",
        "\n",
        "# 4. Start FastAPI in the background via Uvicorn\n",
        "print(\"Starting FastAPI server in the background...\")\n",
        "os.system(\"nohup python -m uvicorn fastapi_server:app --host 0.0.0.0 --port 8000 > fastapi.log 2>&1 &\")\n",
        "\n",
        "# 5. Start Cloudflare Tunnel in the background\n",
        "print(\"Starting Cloudflare Tunnel...\")\n",
        "os.system(\"nohup ./cloudflared tunnel --url http://127.0.0.1:8000 > cloudflare.log 2>&1 &\")\n",
        "\n",
        "# Wait a few seconds for Cloudflare to assign a URL\n",
        "print(\"Waiting for URL...\")\n",
        "time.sleep(8)\n",
        "\n",
        "# 6. Read the log to extract the URL\n",
        "with open(\"cloudflare.log\", \"r\") as f:\n",
        "    logs = f.read()\n",
        "    match = re.search(r\"(https://[a-zA-Z0-9-]+\\.trycloudflare\\.com)\", logs)\n",
        "\n",
        "    if match:\n",
        "        public_url = match.group(1)\n",
        "        base_url = f\"{public_url}/v1\"\n",
        "\n",
        "        # Save the URL to a file\n",
        "        with open(\"api_url.txt\", \"w\") as url_file:\n",
        "            url_file.write(base_url)\n",
        "\n",
        "        print(f\"\\n‚úÖ URL saved to api_url.txt\")\n",
        "        print(f\"üëâ {base_url}\\n\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Could not find Cloudflare URL.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObklTeZdLE3i",
        "outputId": "1d2bb929-8df9-42af-ff69-5a6a2b3cde68"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting FastAPI server in the background...\n",
            "Starting Cloudflare Tunnel...\n",
            "Waiting for URL...\n",
            "\n",
            "‚úÖ URL saved to api_url.txt\n",
            "üëâ https://editorial-details-updating-turns.trycloudflare.com/v1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Abaixo est√° um exemplo de uso da API, pode ser usado de qualquer computador, basta preencher o API_BASE_URL com a URL do servidor da c√©lula acima"
      ],
      "metadata": {
        "id": "9aoJa3MgSadk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test your API with the official OpenAI Python package\n",
        "from openai import OpenAI\n",
        "\n",
        "# Read the base URL automatically from the file\n",
        "with open(\"api_url.txt\", \"r\") as f:\n",
        "    API_BASE_URL = f.read().strip()\n",
        "\n",
        "print(f\"Connecting to: {API_BASE_URL}\\n\")\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=API_BASE_URL,\n",
        "    api_key=\"sk-no-key-required\"\n",
        ")\n",
        "\n",
        "\n",
        "# --- 1. GET MODELS ---\n",
        "print(\"Fetching models...\")\n",
        "models = client.models.list()\n",
        "print(f\"Available models: {[m.id for m in models.data]}\\n\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 2. STREAMING COMPLETION ---\n",
        "print(\"Sending chat request (Streaming)...\\n\")\n",
        "stream_response = client.chat.completions.create(\n",
        "    model=\"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explique o que √© um llamacpp server e o que √© um Cloudflared tunnel\"}\n",
        "    ],\n",
        "    stream=True # <--- Set to True\n",
        ")\n",
        "\n",
        "# Print the streaming response as it arrives\n",
        "for chunk in stream_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n\" + \"-\" * 50)\n",
        "\n",
        "\n",
        "# --- 3. NON-STREAMING COMPLETION ---\n",
        "print(\"Sending chat request (Non-Streaming)...\\n\")\n",
        "standard_response = client.chat.completions.create(\n",
        "    model=\"unsloth/Qwen3.5-27B-GGUF\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful and concise AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"O que √© aux√≠lio-doen√ßa no direito brasileiro ? N√£o use markdown na resposta\"}\n",
        "    ],\n",
        "    stream=False # <--- Set to False\n",
        ")\n",
        "\n",
        "# Print the final complete message\n",
        "print(standard_response.choices[0].message.content)\n",
        "print(\"\\n\" + \"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nundtfm5LMFi",
        "outputId": "b9b57d2f-8aeb-46f2-a7fd-dc00fcfd56a2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connecting to: https://editorial-details-updating-turns.trycloudflare.com/v1\n",
            "\n",
            "Fetching models...\n",
            "Available models: ['unsloth/Qwen3.5-27B-GGUF:UD-Q4_K_XL']\n",
            "\n",
            "--------------------------------------------------\n",
            "Sending chat request (Streaming)...\n",
            "\n",
            "Aqui est√° uma explica√ß√£o concisa de cada conceito e como eles se relacionam:\n",
            "\n",
            "### 1. Llama.cpp Server\n",
            "O **Llama.cpp** √© uma biblioteca de c√≥digo aberto escrita em C/C++ otimizada para rodar modelos de linguagem grandes (LLMs) como Llama, Mistral e Gemma em hardware local (CPU ou GPU de consumidor), sem precisar de grandes clusters de servidores.\n",
            "\n",
            "O **Llama.cpp Server** √© uma funcionalidade espec√≠fica dentro dessa biblioteca que transforma o modelo local em um **servi√ßo de API web** (geralmente compat√≠vel com a API da OpenAI).\n",
            "*   **Como funciona:** Ele inicia um servidor local (ex: `localhost:8080`) que aceita requisi√ß√µes HTTP para gerar texto, completar prompts ou chat.\n",
            "*   **Uso principal:** Permite que aplica√ß√µes, scripts ou interfaces de usu√°rio (como o Ollama ou LM Studio) se comuniquem com o modelo rodando no seu computador como se fosse uma API externa.\n",
            "*   **Limita√ß√£o:** Por padr√£o, esse servidor s√≥ √© acess√≠vel dentro da sua rede local (localhost).\n",
            "\n",
            "### 2. Cloudflared Tunnel\n",
            "O **Cloudflared** √© uma ferramenta de linha de comando (cliente) fornecida pela Cloudflare. Ele cria um **t√∫nel seguro e criptografado** entre um servidor local (que n√£o tem um IP p√∫blico ou est√° atr√°s de um firewall/NAT) e a rede global da Cloudflare.\n",
            "\n",
            "*   **Como funciona:** Ele estabelece uma conex√£o persistente de sa√≠da do seu computador para os servidores da Cloudflare. Isso permite que voc√™ exponha servi√ßos locais para a internet sem precisar configurar portas no roteador (port forwarding) ou ter um IP p√∫blico fixo.\n",
            "*   **Seguran√ßa:** O tr√°fego √© criptografado e passa pela rede de seguran√ßa da Cloudflare (prote√ß√£o DDoS, WAF), ocultando o IP real do seu servidor.\n",
            "*   **Resultado:** Voc√™ recebe um dom√≠nio p√∫blico (ex: `meu-modelo.trycloudflare.com`) que redireciona o tr√°fego para o seu servi√ßo local.\n",
            "\n",
            "---\n",
            "\n",
            "### Como eles funcionam juntos\n",
            "A combina√ß√£o desses dois √© muito popular para **hospedagem de LLMs locais acess√≠veis remotamente**:\n",
            "\n",
            "1.  Voc√™ roda o **Llama.cpp Server** no seu computador para processar o modelo.\n",
            "2.  Voc√™ usa o **Cloudflared Tunnel** para criar uma \"ponte\" segura que exp√µe esse servidor local para a internet.\n",
            "3.  **Resultado:** Voc√™ pode acessar sua API de IA de qualquer lugar do mundo (celular, outro computador) atrav√©s de um dom√≠nio seguro, sem abrir portas vulner√°veis no seu roteador.\n",
            "\n",
            "**Exemplo de fluxo:**\n",
            "`Usu√°rio na Internet` -> `Cloudflare (T√∫nel)` -> `Seu Computador (Llama.cpp Server)` -> `Resposta do Modelo`.\n",
            "\n",
            "--------------------------------------------------\n",
            "Sending chat request (Non-Streaming)...\n",
            "\n",
            "O aux√≠lio-doen√ßa √© um benef√≠cio previdenci√°rio do Instituto Nacional do Seguro Social (INSS) no Brasil, destinado ao segurado que se incapacita temporariamente para o trabalho ou para suas atividades habituais devido a uma doen√ßa ou acidente, desde que n√£o seja decorrente de ato de guerra ou risco profissional espec√≠fico (que geraria aux√≠lio-acidente ou aposentadoria por invalidez). Para ter direito, o segurado precisa cumprir um per√≠odo de car√™ncia, geralmente de 12 meses de contribui√ß√µes, salvo em casos de acidente de qualquer natureza ou doen√ßa profissional e do trabalho. O benef√≠cio √© pago mensalmente e cessa quando o segurado se recupera, retorna ao trabalho ou √© aposentado por invalidez. A concess√£o depende de per√≠cia m√©dica do INSS que comprove a incapacidade tempor√°ria.\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}